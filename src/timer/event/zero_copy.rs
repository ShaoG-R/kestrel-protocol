use crate::timer::TimerEventData;
use crate::timer::event::lockfree_ring::RingBuffer;
use crate::timer::event::memory_pool::OptimizedBatchProcessor;
use crate::timer::event::traits::EventDataTrait;
use std::marker::PhantomData;
use std::sync::atomic::{AtomicUsize, Ordering};

/// é›¶æ‹·è´äº‹ä»¶ä¼ é€’æ¥å£
/// Zero-copy event delivery interface
pub trait ZeroCopyEventDelivery<E: EventDataTrait> {
    /// ç›´æ¥ä¼ é€’äº‹ä»¶å¼•ç”¨ï¼Œé¿å…å…‹éš†
    /// Directly deliver event reference, avoiding clones
    /// æ³¨æ„ï¼šå½“å‰ä¸»è¦ç”¨äºtraitå®Œæ•´æ€§ï¼Œæœªæ¥å¯èƒ½ç”¨äºå•äº‹ä»¶ä¼˜åŒ–
    /// Note: Currently used for trait completeness, may be used for single-event optimization in the future
    #[allow(dead_code)]
    fn deliver_event_ref(&self, event_ref: &TimerEventData<E>) -> bool;

    /// æ‰¹é‡ä¼ é€’äº‹ä»¶å¼•ç”¨
    /// Batch deliver event references
    fn batch_deliver_event_refs(&self, event_refs: &[&TimerEventData<E>]) -> usize;
}

/// äº‹ä»¶æ§½ä½ï¼ˆåŸºäºæ— é”é˜Ÿåˆ—ï¼‰
/// Event slot (based on lock-free queue)
pub struct EventSlot<E: EventDataTrait> {
    /// æ— é”ç¯å½¢ç¼“å†²åŒº
    /// Lock-free ring buffer
    ring_buffer: RingBuffer<E>,
    /// æ€§èƒ½ç»Ÿè®¡
    /// Performance statistics
    written_count: AtomicUsize,
    failed_writes: AtomicUsize,
}

impl<E: EventDataTrait> EventSlot<E> {
    /// åˆ›å»ºæ–°çš„äº‹ä»¶æ§½ä½
    /// Create new event slot
    pub fn new(capacity: usize) -> Self {
        Self {
            ring_buffer: RingBuffer::new(capacity),
            written_count: AtomicUsize::new(0),
            failed_writes: AtomicUsize::new(0),
        }
    }

    /// é›¶æ‹·è´å†™å…¥äº‹ä»¶ï¼ˆç§»åŠ¨è¯­ä¹‰ï¼‰
    /// Zero-copy write event (move semantics)
    #[inline(always)]
    #[allow(dead_code)]
    pub fn write_event(&self, event: TimerEventData<E>) -> bool {
        match self.ring_buffer.try_write(event) {
            Ok(()) => {
                self.written_count.fetch_add(1, Ordering::Relaxed);
                true
            }
            Err(_) => {
                self.failed_writes.fetch_add(1, Ordering::Relaxed);
                false
            }
        }
    }

    /// å°è¯•å†™å…¥äº‹ä»¶ï¼Œå¤±è´¥æ—¶è¿”å›åŸå§‹äº‹ä»¶ï¼ˆç”¨äºé‡è¯•ï¼‰
    /// Try to write event, return original event on failure (for retry)
    #[inline(always)]
    pub fn try_write_event(&self, event: TimerEventData<E>) -> Result<(), TimerEventData<E>> {
        match self.ring_buffer.try_write(event) {
            Ok(()) => {
                self.written_count.fetch_add(1, Ordering::Relaxed);
                Ok(())
            }
            Err(event) => {
                self.failed_writes.fetch_add(1, Ordering::Relaxed);
                Err(event)
            }
        }
    }

    /// æ‰¹é‡å†™å…¥äº‹ä»¶ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
    /// Batch write events (high-performance version)
    #[inline(always)]
    #[allow(dead_code)]
    pub fn batch_write_events(&self, events: Vec<TimerEventData<E>>) -> usize {
        let written = self.ring_buffer.batch_write(events);
        self.written_count.fetch_add(written, Ordering::Relaxed);
        written
    }

    /// å°è¯•è¯»å–äº‹ä»¶
    /// Try to read event
    #[inline(always)]
    #[allow(dead_code)]
    pub fn try_read(&self) -> Option<TimerEventData<E>> {
        self.ring_buffer.try_read()
    }

    /// æ‰¹é‡è¯»å–äº‹ä»¶
    /// Batch read events
    #[inline(always)]
    #[allow(dead_code)]
    pub fn batch_read(&self, max_events: usize) -> Vec<TimerEventData<E>> {
        self.ring_buffer.batch_read(max_events)
    }

    /// è·å–æ€§èƒ½ç»Ÿè®¡
    /// Get performance statistics
    pub fn get_stats(&self) -> (usize, usize, f64) {
        let written = self.written_count.load(Ordering::Relaxed);
        let failed = self.failed_writes.load(Ordering::Relaxed);
        let utilization = self.ring_buffer.utilization();
        (written, failed, utilization)
    }
}
/// å¼•ç”¨ä¼ é€’äº‹ä»¶å¤„ç†å™¨
/// Reference-passing event handler
pub struct RefEventHandler<E: EventDataTrait, F>
where
    F: Fn(&TimerEventData<E>) -> bool + Send + Sync,
{
    handler: F,
    processed_count: AtomicUsize,
    marker: PhantomData<E>,
}

impl<E: EventDataTrait, F> RefEventHandler<E, F>
where
    F: Fn(&TimerEventData<E>) -> bool + Send + Sync,
{
    pub fn new(handler: F) -> Self {
        Self {
            handler,
            processed_count: AtomicUsize::new(0),
            marker: PhantomData,
        }
    }
}

impl<E: EventDataTrait, F> ZeroCopyEventDelivery<E> for RefEventHandler<E, F>
where
    F: Fn(&TimerEventData<E>) -> bool + Send + Sync,
{
    fn deliver_event_ref(&self, event_ref: &TimerEventData<E>) -> bool {
        let success = (self.handler)(event_ref);
        if success {
            self.processed_count.fetch_add(1, Ordering::Relaxed);
        }
        success
    }

    fn batch_deliver_event_refs(&self, event_refs: &[&TimerEventData<E>]) -> usize {
        let mut delivered_count = 0;
        for event_ref in event_refs {
            if (self.handler)(event_ref) {
                delivered_count += 1;
            }
        }
        self.processed_count
            .fetch_add(delivered_count, Ordering::Relaxed);
        delivered_count
    }
}

/// æ™ºèƒ½é›¶æ‹·è´æ‰¹é‡åˆ†å‘å™¨ï¼ˆé›†æˆä¼˜åŒ–æ‰¹é‡å¤„ç†å™¨å’Œæ— é”é˜Ÿåˆ—ï¼‰
/// Smart zero-copy batch dispatcher (integrated with optimized batch processor and lock-free queues)
pub struct SmartZeroCopyDispatcher<E: EventDataTrait>
where
    E: Default + Clone,
{
    /// äº‹ä»¶æ§½ä½
    /// Event slots
    event_slots: Vec<EventSlot<E>>,
    /// ä¼˜åŒ–æ‰¹é‡å¤„ç†å™¨ï¼ˆé›†æˆäº†äº‹ä»¶å·¥å‚å’Œå†…å­˜æ± ï¼‰
    /// Optimized batch processor (integrated with event factory and memory pool)
    batch_processor: OptimizedBatchProcessor<E>,
    /// åˆ†å‘ç´¢å¼•ï¼ˆæ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼‰
    /// Dispatch index (smart load balancing)
    dispatch_index: AtomicUsize,
    /// æ‰¹é‡å¤„ç†é˜ˆå€¼
    /// Batch processing threshold
    batch_threshold: usize,
    /// æ€§èƒ½ç»Ÿè®¡
    /// Performance statistics
    total_dispatched: AtomicUsize,
    total_rejected: AtomicUsize,
}

impl<E: EventDataTrait> SmartZeroCopyDispatcher<E>
where
    E: Default + Clone,
{
    /// åˆ›å»ºæ™ºèƒ½é›¶æ‹·è´åˆ†å‘å™¨
    /// Create smart zero-copy dispatcher
    pub fn new(slot_capacity: usize, dispatcher_count: usize, pool_size: usize) -> Self {
        let mut event_slots = Vec::with_capacity(dispatcher_count);
        for _ in 0..dispatcher_count {
            event_slots.push(EventSlot::new(slot_capacity));
        }

        let batch_threshold = 32; // æ™ºèƒ½æ‰¹é‡å¤„ç†é˜ˆå€¼
        let batch_processor = OptimizedBatchProcessor::new(pool_size, batch_threshold);

        Self {
            event_slots,
            batch_processor,
            dispatch_index: AtomicUsize::new(0),
            batch_threshold,
            total_dispatched: AtomicUsize::new(0),
            total_rejected: AtomicUsize::new(0),
        }
    }

    /// é«˜æ€§èƒ½æ‰¹é‡äº‹ä»¶åˆ†å‘ï¼ˆæ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼‰
    /// High-performance batch event dispatch (smart load balancing)
    pub fn batch_dispatch_events(&self, events: Vec<TimerEventData<E>>) -> usize {
        if events.is_empty() {
            return 0;
        }

        let events_len = events.len();
        let dispatcher_count = self.event_slots.len();

        // æ™ºèƒ½é€‰æ‹©æœ€ä½³åˆ†å‘å™¨ï¼ˆåŸºäºåˆ©ç”¨ç‡ï¼‰
        // Smart selection of best dispatcher (based on utilization)
        let start_index = self.dispatch_index.load(Ordering::Relaxed);
        let mut best_dispatcher = start_index % dispatcher_count;
        let mut min_utilization = f64::MAX;

        // å¿«é€Ÿæ£€æŸ¥å‰3ä¸ªåˆ†å‘å™¨ï¼Œé€‰æ‹©åˆ©ç”¨ç‡æœ€ä½çš„
        // Quick check of first 3 dispatchers, select the one with lowest utilization
        for i in 0..std::cmp::min(3, dispatcher_count) {
            let idx = (start_index + i) % dispatcher_count;
            let (_, _, utilization) = self.event_slots[idx].get_stats();
            if utilization < min_utilization {
                min_utilization = utilization;
                best_dispatcher = idx;
            }
        }

        // æ™ºèƒ½åˆ†å‘ï¼šæ ¹æ®æ‰¹é‡å¤§å°é€‰æ‹©ä¸åŒç­–ç•¥
        // Smart dispatch: choose different strategy based on batch size
        let dispatched = if events_len >= self.batch_threshold {
            // å¤§æ‰¹é‡ï¼šä¼˜å…ˆå¡«æ»¡æœ€ä½³åˆ†å‘å™¨ï¼Œç„¶ååˆ†æ•£åˆ°å…¶ä»–åˆ†å‘å™¨
            // Large batch: fill best dispatcher first, then distribute to others
            self.dispatch_large_batch(events, best_dispatcher, dispatcher_count)
        } else {
            // å°æ‰¹é‡ï¼šè½®è¯¢åˆ†å‘ä»¥ç¡®ä¿å»¶è¿Ÿæœ€ä½
            // Small batch: round-robin dispatch for minimum latency
            self.dispatch_small_batch(events, best_dispatcher, dispatcher_count)
        };

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        // Update statistics
        self.total_dispatched
            .fetch_add(dispatched, Ordering::Relaxed);
        self.total_rejected
            .fetch_add(events_len - dispatched, Ordering::Relaxed);

        // æ›´æ–°åˆ†å‘ç´¢å¼•
        // Update dispatch index
        self.dispatch_index
            .store((best_dispatcher + 1) % dispatcher_count, Ordering::Relaxed);

        dispatched
    }

    /// å¤§æ‰¹é‡åˆ†å‘ç­–ç•¥ï¼šæ™ºèƒ½åˆ†æ•£åˆ°å¤šä¸ªåˆ†å‘å™¨
    /// Large batch dispatch strategy: intelligently distribute to multiple dispatchers
    fn dispatch_large_batch(
        &self,
        events: Vec<TimerEventData<E>>,
        best_dispatcher: usize,
        dispatcher_count: usize,
    ) -> usize {
        let events_per_dispatcher = events.len() / dispatcher_count;
        let mut total_dispatched = 0;
        let mut event_iter = events.into_iter();

        // é¦–å…ˆå°è¯•åœ¨æœ€ä½³åˆ†å‘å™¨ä¸Šå¤„ç†æ›´å¤šäº‹ä»¶
        // First try to handle more events on the best dispatcher
        let best_batch_size = std::cmp::min(events_per_dispatcher * 2, event_iter.len());
        let mut best_batch = Vec::with_capacity(best_batch_size);

        for _ in 0..best_batch_size {
            if let Some(event) = event_iter.next() {
                best_batch.push(event);
            } else {
                break;
            }
        }

        if !best_batch.is_empty() {
            total_dispatched += self.event_slots[best_dispatcher].batch_write_events(best_batch);
        }

        // å°†å‰©ä½™äº‹ä»¶åˆ†æ•£åˆ°å…¶ä»–åˆ†å‘å™¨
        // Distribute remaining events to other dispatchers
        let mut current_dispatcher = (best_dispatcher + 1) % dispatcher_count;
        let mut current_batch = Vec::with_capacity(events_per_dispatcher);

        for event in event_iter {
            current_batch.push(event);

            // å½“æ‰¹é‡è¾¾åˆ°åˆé€‚å¤§å°æˆ–æ²¡æœ‰æ›´å¤šäº‹ä»¶æ—¶ï¼Œå†™å…¥å½“å‰åˆ†å‘å™¨
            // Write to current dispatcher when batch reaches appropriate size or no more events
            if current_batch.len() >= events_per_dispatcher {
                total_dispatched +=
                    self.event_slots[current_dispatcher].batch_write_events(current_batch);
                current_batch = Vec::with_capacity(events_per_dispatcher);
                current_dispatcher = (current_dispatcher + 1) % dispatcher_count;
            }
        }

        // å¤„ç†å‰©ä½™çš„äº‹ä»¶
        // Handle remaining events
        if !current_batch.is_empty() {
            total_dispatched +=
                self.event_slots[current_dispatcher].batch_write_events(current_batch);
        }

        total_dispatched
    }

    /// å°æ‰¹é‡åˆ†å‘ç­–ç•¥ï¼šè½®è¯¢åˆ†å‘ç¡®ä¿ä½å»¶è¿Ÿ
    /// Small batch dispatch strategy: round-robin for low latency
    fn dispatch_small_batch(
        &self,
        events: Vec<TimerEventData<E>>,
        best_dispatcher: usize,
        dispatcher_count: usize,
    ) -> usize {
        let mut dispatched_count = 0;
        let mut current_dispatcher = best_dispatcher;

        for mut event in events {
            let mut placed = false;
            let original_dispatcher = current_dispatcher;

            // å°è¯•æ‰€æœ‰åˆ†å‘å™¨ï¼ˆæ¯ä¸ªäº‹ä»¶åªè¯•ä¸€è½®ï¼‰
            // Try all dispatchers (only one round per event)
            loop {
                match self.event_slots[current_dispatcher].try_write_event(event) {
                    Ok(()) => {
                        dispatched_count += 1;
                        placed = true;
                        current_dispatcher = (current_dispatcher + 1) % dispatcher_count;
                        break;
                    }
                    Err(returned_event) => {
                        event = returned_event;
                        current_dispatcher = (current_dispatcher + 1) % dispatcher_count;

                        // å¦‚æœå›åˆ°èµ·å§‹åˆ†å‘å™¨ï¼Œè¯´æ˜æ‰€æœ‰åˆ†å‘å™¨éƒ½æ»¡äº†
                        // If we're back to the starting dispatcher, all are full
                        if current_dispatcher == original_dispatcher {
                            break;
                        }
                    }
                }
            }

            // å¦‚æœæ— æ³•æ”¾ç½®äº‹ä»¶ï¼Œåœæ­¢å°è¯•åç»­äº‹ä»¶
            // If unable to place event, stop trying subsequent events
            if !placed {
                break;
            }
        }

        dispatched_count
    }

    /// æ™ºèƒ½æ‰¹é‡åˆ›å»ºå¹¶åˆ†å‘äº‹ä»¶ï¼ˆä½¿ç”¨ä¼˜åŒ–æ‰¹é‡å¤„ç†å™¨ï¼‰
    /// Smart batch create and dispatch events (using optimized batch processor)
    pub fn create_and_dispatch_events(&self, requests: &[(u32, E)]) -> usize {
        if requests.is_empty() {
            return 0;
        }

        // å¯¹äºå°æ‰¹é‡ï¼Œç›´æ¥åˆ›å»ºäº‹ä»¶é¿å…æ‰¹é‡å¤„ç†å™¨çš„å¼€é”€
        // For small batches, create events directly to avoid batch processor overhead
        if requests.len() <= 64 {
            let events: Vec<_> = requests
                .iter()
                .map(|(connection_id, event_data)| {
                    TimerEventData::new(*connection_id, event_data.clone())
                })
                .collect();
            return self.batch_dispatch_events(events);
        }

        // å¤§æ‰¹é‡ä½¿ç”¨ä¼˜åŒ–æ‰¹é‡å¤„ç†å™¨
        // Use optimized batch processor for large batches
        let events = self.batch_processor.create_events_optimized(requests);
        self.batch_dispatch_events(events)
    }

    /// æ‰¹é‡æ¶ˆè´¹äº‹ä»¶ï¼ˆç”¨äºæµ‹è¯•å’Œç›‘æ§ï¼‰
    /// Batch consume events (for testing and monitoring)
    pub fn batch_consume_events(&self, max_events_per_slot: usize) -> Vec<TimerEventData<E>> {
        let mut all_events = Vec::new();

        for slot in &self.event_slots {
            let events = slot.batch_read(max_events_per_slot);
            all_events.extend(events);
        }

        all_events
    }

    /// æ‰¹é‡å½’è¿˜äº‹ä»¶åˆ°å†…å­˜æ± 
    /// Batch return events to memory pool
    pub fn batch_return_to_pool(&self, events: Vec<TimerEventData<E>>) {
        self.batch_processor.process_completed(events);
    }

    /// è·å–è¯¦ç»†æ€§èƒ½ç»Ÿè®¡
    /// Get detailed performance statistics
    #[cfg(test)]
    pub fn get_detailed_stats(&self) -> DetailedPerformanceStats {
        let dispatched = self.total_dispatched.load(Ordering::Relaxed);
        let rejected = self.total_rejected.load(Ordering::Relaxed);

        // è·å–æ‰¹é‡å¤„ç†å™¨çš„æ€§èƒ½ç»Ÿè®¡
        // Get batch processor performance statistics
        let (_processed_events, pool_size, _, _, large_batch_ratio) =
            self.batch_processor.get_performance_stats();

        let avg_utilization = self
            .event_slots
            .iter()
            .map(|slot| {
                let (_, _, util) = slot.get_stats();
                util
            })
            .sum::<f64>()
            / self.event_slots.len() as f64;

        DetailedPerformanceStats {
            total_dispatched: dispatched,
            total_rejected: rejected,
            success_rate: if dispatched + rejected > 0 {
                dispatched as f64 / (dispatched + rejected) as f64
            } else {
                1.0
            },
            memory_pool_hit_rate: large_batch_ratio, // å¤§æ‰¹é‡å¤„ç†ç‡ä½œä¸ºå‘½ä¸­ç‡æŒ‡æ ‡
            memory_pool_size: pool_size,
            memory_pool_reuse_rate: large_batch_ratio, // å¤§æ‰¹é‡å¤„ç†ç‡ä½œä¸ºå¤ç”¨ç‡æŒ‡æ ‡
            average_slot_utilization: avg_utilization,
            dispatcher_count: self.event_slots.len(),
        }
    }
}

/// è¯¦ç»†æ€§èƒ½ç»Ÿè®¡ç»“æ„
/// Detailed performance statistics structure
#[derive(Debug, Clone)]
#[cfg(test)]
pub struct DetailedPerformanceStats {
    pub total_dispatched: usize,
    pub total_rejected: usize,
    pub success_rate: f64,
    pub memory_pool_hit_rate: f64,
    pub memory_pool_size: usize,
    pub memory_pool_reuse_rate: f64,
    pub average_slot_utilization: f64,
    pub dispatcher_count: usize,
}
#[cfg(test)]
impl DetailedPerformanceStats {
    /// æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    /// Print detailed statistics
    pub fn print_summary(&self) {
        println!("=== æ™ºèƒ½é›¶æ‹·è´åˆ†å‘å™¨æ€§èƒ½ç»Ÿè®¡ ===");
        println!("ğŸ“Š äº‹ä»¶åˆ†å‘ç»Ÿè®¡:");
        println!("  - æˆåŠŸåˆ†å‘: {}", self.total_dispatched);
        println!("  - åˆ†å‘å¤±è´¥: {}", self.total_rejected);
        println!("  - æˆåŠŸç‡: {:.1}%", self.success_rate * 100.0);
        println!("ğŸ”‹ å†…å­˜æ± ç»Ÿè®¡:");
        println!("  - æ± å‘½ä¸­ç‡: {:.1}%", self.memory_pool_hit_rate * 100.0);
        println!("  - å½“å‰æ± å¤§å°: {}", self.memory_pool_size);
        println!("  - å¤ç”¨ç‡: {:.1}%", self.memory_pool_reuse_rate * 100.0);
        println!("âš¡ åˆ†å‘å™¨ç»Ÿè®¡:");
        println!(
            "  - å¹³å‡åˆ©ç”¨ç‡: {:.1}%",
            self.average_slot_utilization * 100.0
        );
        println!("  - åˆ†å‘å™¨æ•°é‡: {}", self.dispatcher_count);
        println!();
    }
}

/// å…¼å®¹æ€§åˆ«åï¼Œä¿æŒå‘åå…¼å®¹
/// Compatibility alias for backward compatibility
pub type ZeroCopyBatchDispatcher<E> = SmartZeroCopyDispatcher<E>;
