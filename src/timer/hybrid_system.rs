//! æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿé›†æˆ
//! Hybrid Parallel Timer System Integration
//!
//! è¯¥æ¨¡å—æä¾›äº†ä½¿ç”¨HybridParallelTimerSystemçš„æ–°å®šæ—¶å™¨ä»»åŠ¡ç³»ç»Ÿï¼Œ
//! ä½œä¸ºGlobalTimerTaskçš„é«˜æ€§èƒ½æ›¿ä»£æ–¹æ¡ˆã€‚
//!
//! This module provides a new timer task system using HybridParallelTimerSystem
//! as a high-performance replacement for GlobalTimerTask.

use crate::timer::{
    event::{TimerEvent, TimerEventId, ConnectionId},
    wheel::{TimingWheel, TimerEntry, TimerEntryId},
    parallel::core::HybridParallelTimerSystem,
    task::{
        commands::{TimerTaskCommand, TimerError},
        types::{
            TimerRegistration, BatchTimerRegistration, BatchTimerCancellation,
            BatchTimerResult, TimerHandle
        }
    },
    TimerTaskStats
};
use crate::timer::event::traits::{EventDataTrait, EventFactory};
use std::collections::{HashMap, HashSet};
use std::time::Duration;

/// é«˜æ€§èƒ½æ‰¹é‡å¤„ç†ç¼“å†²åŒºï¼ˆæœ¬åœ°å®šä¹‰ï¼‰
/// High-performance batch processing buffers (local definition)
struct HybridBatchProcessingBuffers {
    /// æŒ‰è¿æ¥åˆ†ç»„çš„è¿‡æœŸå®šæ—¶å™¨IDï¼ˆé¢„åˆ†é…å®¹é‡ï¼‰
    /// Expired timer IDs grouped by connection (pre-allocated capacity)
    expired_by_connection: HashMap<ConnectionId, Vec<TimerEntryId>>,
    /// é¢„åˆ†é…çš„æ¡ç›®IDç¼“å†²åŒºç”¨äºæ‰¹é‡ç§»é™¤
    /// Pre-allocated entry ID buffer for batch removal
    entry_ids_buffer: Vec<TimerEntryId>,
    /// è¿æ¥æ˜ å°„æ›´æ–°ç¼“å†²åŒº
    /// Connection mapping update buffer
    connection_updates: HashMap<ConnectionId, Vec<TimerEntryId>>,
}

impl HybridBatchProcessingBuffers {
    fn new() -> Self {
        Self {
            expired_by_connection: HashMap::with_capacity(64),
            entry_ids_buffer: Vec::with_capacity(256),
            connection_updates: HashMap::with_capacity(64),
        }
    }
    
    fn clear(&mut self) {
        // æ¸…ç©ºä½†ä¿ç•™å®¹é‡ä»¥é¿å…é‡æ–°åˆ†é…
        // Clear but retain capacity to avoid reallocation
        self.expired_by_connection.clear();
        self.entry_ids_buffer.clear(); 
        self.connection_updates.clear();
        
        // å¦‚æœç¼“å†²åŒºè¿‡å¤§ï¼Œé€‚åº¦æ”¶ç¼©ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
        // If buffers are too large, shrink moderately to control memory usage
        if self.expired_by_connection.capacity() > 512 {
            self.expired_by_connection.shrink_to(256);
        }
        if self.entry_ids_buffer.capacity() > 2048 {
            self.entry_ids_buffer.shrink_to(1024);
        }
        if self.connection_updates.capacity() > 512 {
            self.connection_updates.shrink_to(256);
        }
    }
}
use tokio::{
    sync::mpsc,
    time::{Instant, interval, sleep_until},
};
use tracing::{debug, info, trace, warn};

/// æ··åˆå¹¶è¡Œå®šæ—¶å™¨ä»»åŠ¡
/// Hybrid parallel timer task
pub struct HybridTimerTask<E: EventDataTrait> {
    /// æ—¶é—´è½®ï¼ˆä¿æŒç°æœ‰çš„å®šæ—¶å™¨ç®¡ç†é€»è¾‘ï¼‰
    /// Timing wheel (maintain existing timer management logic)
    timing_wheel: TimingWheel<E>,
    /// æ··åˆå¹¶è¡Œå¤„ç†ç³»ç»Ÿ
    /// Hybrid parallel processing system
    parallel_system: HybridParallelTimerSystem<E>,
    /// å‘½ä»¤æ¥æ”¶é€šé“
    /// Command receiver channel
    command_rx: mpsc::Receiver<TimerTaskCommand<E>>,
    /// å‘½ä»¤å‘é€é€šé“ï¼ˆç”¨äºåˆ›å»ºå¥æŸ„ï¼‰
    /// Command sender channel (for creating handles)
    command_tx: mpsc::Sender<TimerTaskCommand<E>>,
    /// è¿æ¥åˆ°å®šæ—¶å™¨æ¡ç›®çš„æ˜ å°„
    /// Connection to timer entries mapping
    connection_timers: HashMap<ConnectionId, HashSet<TimerEntryId>>,
    /// å®šæ—¶å™¨æ¡ç›®åˆ°è¿æ¥çš„åå‘æ˜ å°„
    /// Reverse mapping from timer entry to connection
    entry_to_connection: HashMap<TimerEntryId, ConnectionId>,
    /// ä¸‹ä¸€ä¸ªåˆ†é…çš„å®šæ—¶å™¨äº‹ä»¶ID
    /// Next timer event ID to allocate
    next_event_id: TimerEventId,
    /// ç»Ÿè®¡ä¿¡æ¯
    /// Statistics
    stats: TimerTaskStats,
    /// é¢„åˆ†é…çš„å®¹å™¨ï¼Œç”¨äºæ‰¹é‡å¤„ç†
    /// Pre-allocated containers for batch processing
    batch_processing_buffers: HybridBatchProcessingBuffers,
    /// å¹¶è¡Œå¤„ç†çš„æ‰¹é‡å¤§å°é˜ˆå€¼
    /// Batch size threshold for parallel processing
    parallel_threshold: usize,
    /// å®šæ—¶å™¨äº‹ä»¶å·¥å‚ (æ™ºèƒ½ç­–ç•¥é€‰æ‹©)
    /// Timer event factory (smart strategy selection)
    event_factory: EventFactory<E>,
}

impl<E: EventDataTrait> HybridTimerTask<E> {
    /// åˆ›å»ºæ–°çš„æ··åˆå¹¶è¡Œå®šæ—¶å™¨ä»»åŠ¡ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
    /// Create new hybrid parallel timer task (high-performance version)
    pub fn new(command_buffer_size: usize, parallel_threshold: usize) -> (Self, mpsc::Sender<TimerTaskCommand<E>>) {
        let (command_tx, command_rx) = mpsc::channel(command_buffer_size);
        let timing_wheel = TimingWheel::new(512, Duration::from_millis(10));
        let parallel_system = HybridParallelTimerSystem::new();
        
        let wheel_stats = timing_wheel.stats();
        let task = Self {
            timing_wheel,
            parallel_system,
            command_rx,
            command_tx: command_tx.clone(),
            // é¢„åˆ†é…æ›´å¤§çš„å®¹é‡ä»¥å‡å°‘é‡æ–°åˆ†é…
            // Pre-allocate larger capacity to reduce reallocations
            connection_timers: HashMap::with_capacity(128),
            entry_to_connection: HashMap::with_capacity(1024),
            next_event_id: 1,
            stats: TimerTaskStats {
                total_timers: 0,
                active_connections: 0,
                processed_timers: 0,
                cancelled_timers: 0,
                wheel_stats,
            },
            batch_processing_buffers: HybridBatchProcessingBuffers::new(),
            parallel_threshold,
            event_factory: EventFactory::new(), // æ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼Œé›¶é…ç½®
        };

        (task, command_tx)
    }

    /// åˆ›å»ºé»˜è®¤é…ç½®çš„æ··åˆå¹¶è¡Œå®šæ—¶å™¨ä»»åŠ¡
    /// Create hybrid parallel timer task with default configuration
    pub fn new_default() -> (Self, mpsc::Sender<TimerTaskCommand<E>>) {
        // é»˜è®¤å¹¶è¡Œé˜ˆå€¼ä¸º32ï¼Œå³è¶…è¿‡32ä¸ªå®šæ—¶å™¨æ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†
        // Default parallel threshold is 32, use parallel processing when more than 32 timers
        Self::new(1024, 32)
    }

    /// è¿è¡Œå®šæ—¶å™¨ä»»åŠ¡ä¸»å¾ªç¯
    /// Run timer task main loop
    pub async fn run(mut self) {
        info!("Hybrid parallel timer task started");
        
        let mut advance_interval = self.calculate_advance_interval();
        let mut next_interval_update = Instant::now() + Duration::from_secs(1);
        
        loop {
            let next_wakeup = self.get_next_wakeup_time();
            let now = Instant::now();

            tokio::select! {
                // å¤„ç†å‘½ä»¤
                // Process commands
                Some(command) = self.command_rx.recv() => {
                    if !self.handle_command(command).await {
                        break; // æ”¶åˆ°å…³é—­å‘½ä»¤
                    }
                    // å‘½ä»¤å¤„ç†åå¯èƒ½å½±å“å®šæ—¶å™¨ï¼Œé‡æ–°è®¡ç®—é—´éš”
                    // After command processing, timers might change, recalculate interval
                    if now >= next_interval_update {
                        advance_interval = self.calculate_advance_interval();
                        next_interval_update = now + Duration::from_secs(1);
                    }
                }
                
                // åŠ¨æ€é—´éš”æ¨è¿›æ—¶é—´è½®
                // Dynamic interval advance timing wheel
                _ = advance_interval.tick() => {
                    self.advance_timing_wheel_hybrid().await;
                }
                
                // åŸºäºæœ€æ—©å®šæ—¶å™¨çš„ç²¾ç¡®å”¤é†’
                // Precise wakeup based on earliest timer
                _ = sleep_until(next_wakeup) => {
                    self.advance_timing_wheel_hybrid().await;
                }
                
                // å®šæœŸæ›´æ–°æ¨è¿›é—´éš”
                // Periodically update advance interval
                _ = sleep_until(next_interval_update) => {
                    advance_interval = self.calculate_advance_interval();
                    next_interval_update = now + Duration::from_secs(1);
                }
            }
        }

        info!("Hybrid parallel timer task shutdown completed");
    }

    /// é«˜æ€§èƒ½æ··åˆå¹¶è¡Œæ¨è¿›æ—¶é—´è½®ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
    /// High-performance hybrid parallel advance timing wheel (core method)
    async fn advance_timing_wheel_hybrid(&mut self) {
        let now = Instant::now();
        let expired_timers = self.timing_wheel.advance(now);

        if expired_timers.is_empty() {
            return;
        }

        let batch_size = expired_timers.len();
        
        // æ¸…ç©ºå¹¶é‡ç”¨ç¼“å†²åŒº
        // Clear and reuse buffers
        self.batch_processing_buffers.clear();

        // å•æ¬¡éå†ä¼˜åŒ–ï¼šåŒæ—¶æ”¶é›†è¿æ¥æ˜ å°„ä¿¡æ¯å’Œé¢„æå–é€šçŸ¥æ•°æ®ï¼Œå‡å°‘éå†æ¬¡æ•°
        // Single-pass optimization: collect connection mapping and pre-extract notification data to reduce iterations
        let mut notification_data = Vec::with_capacity(batch_size);
        self.batch_processing_buffers.entry_ids_buffer.reserve(batch_size);
        
        for entry in &expired_timers {
            let entry_id = entry.id;
            
            // é¢„æå–é€šçŸ¥æ•°æ®ï¼ˆé¿å…åç»­çš„ cloneï¼‰
            // Pre-extract notification data (avoid subsequent clones)
            notification_data.push((
                entry_id,
                entry.event.data.connection_id,
                entry.event.data.timeout_event.clone(),
                entry.event.callback_tx.clone(),
                entry.event.data.clone(),
            ));
            
            // æ·»åŠ åˆ° ID ç¼“å†²åŒºç”¨äºæ‰¹é‡ç§»é™¤
            // Add to ID buffer for batch removal
            self.batch_processing_buffers.entry_ids_buffer.push(entry_id);
            
            // æ”¶é›†è¿æ¥æ˜ å°„ä¿¡æ¯ï¼ˆä¼˜åŒ–çš„å•æ¬¡æŸ¥æ‰¾å’Œç§»é™¤ï¼‰
            // Collect connection mapping info (optimized single lookup and removal)
            if let Some(conn_id) = self.entry_to_connection.remove(&entry_id) {
                self.batch_processing_buffers.connection_updates
                    .entry(conn_id)
                    .or_insert_with(|| Vec::with_capacity(4))
                    .push(entry_id);
            }
        }

        // æ‰¹é‡æ¸…ç†è¿æ¥å®šæ—¶å™¨æ˜ å°„ï¼ˆä¼˜åŒ–çš„æ‰¹é‡æ›´æ–°ï¼‰
        // Batch cleanup connection timer mappings (optimized batch update)
        for (conn_id, expired_ids) in self.batch_processing_buffers.connection_updates.drain() {
            if let Some(entry_ids) = self.connection_timers.get_mut(&conn_id) {
                // ä½¿ç”¨å‘é‡åŒ–ç§»é™¤æå‡æ€§èƒ½
                // Use vectorized removal for better performance
                for expired_id in &expired_ids {
                    entry_ids.remove(expired_id);
                }
                if entry_ids.is_empty() {
                    self.connection_timers.remove(&conn_id);
                }
            }
        }

        // æ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®æ‰¹é‡å¤§å°å’Œç³»ç»Ÿè´Ÿè½½é€‰æ‹©æœ€ä¼˜å¤„ç†æ–¹å¼
        // Smart strategy selection: choose optimal processing based on batch size and system load
        let should_use_parallel = batch_size >= self.parallel_threshold && 
            self.stats.total_timers >= 100; // é¢å¤–çš„è´Ÿè½½æ£€æŸ¥

        if should_use_parallel {
            // ä½¿ç”¨æ··åˆå¹¶è¡Œç³»ç»Ÿå¤„ç†å¤§æ‰¹é‡
            // Use hybrid parallel system for large batches
            match self.parallel_system.process_timer_batch(expired_timers).await {
                Ok(parallel_result) => {
                    trace!(
                        processed_count = parallel_result.processed_count,
                        strategy = ?parallel_result.strategy_used,
                        duration_us = parallel_result.processing_duration.as_micros(),
                        "Parallel batch processing completed"
                    );
                    
                    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    // Update statistics
                    self.stats.processed_timers += parallel_result.processed_count as u64;
                    
                    // é«˜æ€§èƒ½å¹¶å‘äº‹ä»¶é€šçŸ¥
                    // High-performance concurrent event notifications
                    self.send_timer_notifications_optimized(notification_data).await;
                }
                Err(e) => {
                    warn!(error = ?e, "Parallel processing failed, falling back to sequential");
                    // å¤±è´¥æ—¶ä½¿ç”¨é¢„æå–çš„æ•°æ®è¿›è¡Œé¡ºåºå¤„ç†
                    // Fallback to sequential processing using pre-extracted data
                    self.process_timers_from_notification_data_optimized(notification_data).await;
                }
            }
        } else {
            // å°æ‰¹é‡ç›´æ¥ä½¿ç”¨ä¼˜åŒ–çš„é¡ºåºå¤„ç†
            // Small batches use optimized sequential processing directly
            self.process_timers_from_notification_data_optimized(notification_data).await;
        }
    }

    /// é«˜æ€§èƒ½å‘é€å®šæ—¶å™¨é€šçŸ¥ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// High-performance send timer notifications (optimized version)
    async fn send_timer_notifications_optimized(
        &mut self, 
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let notification_count = notification_data.len();
        
        if notification_count == 0 {
            return;
        }
        
        // åŸºäºæ‰¹é‡å¤§å°é€‰æ‹©æœ€ä¼˜å¹¶å‘ç­–ç•¥
        // Choose optimal concurrency strategy based on batch size
        if notification_count <= 16 {
            // å°æ‰¹é‡ä½¿ç”¨é¡ºåºå‘é€å‡å°‘å¹¶å‘å¼€é”€
            // Small batches use sequential sending to reduce concurrency overhead
            self.send_notifications_sequential(notification_data).await;
        } else {
            // å¤§æ‰¹é‡ä½¿ç”¨åˆ†å—å¹¶å‘å¤„ç†
            // Large batches use chunked concurrent processing
            self.send_notifications_chunked_concurrent(notification_data).await;
        }
    }
    
    /// é¡ºåºå‘é€é€šçŸ¥ï¼ˆå°æ‰¹é‡ä¼˜åŒ–ï¼‰
    /// Sequential notification sending (small batch optimization)
    async fn send_notifications_sequential(
        &mut self,
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let mut successful_count = 0;
        
        for (entry_id, connection_id, event_type, callback_tx, event_data) in notification_data {
            if callback_tx.send(event_data).await.is_ok() {
                successful_count += 1;
                trace!(entry_id, connection_id, event_type = ?event_type, "Timer event sent");
            } else {
                warn!(entry_id, connection_id, "Failed to send timer event");
            }
        }
        
        self.stats.processed_timers += successful_count;
    }
    
    /// åˆ†å—å¹¶å‘å‘é€é€šçŸ¥ï¼ˆå¤§æ‰¹é‡ä¼˜åŒ–ï¼‰
    /// Chunked concurrent notification sending (large batch optimization)
    async fn send_notifications_chunked_concurrent(
        &mut self,
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let notification_count = notification_data.len();
        let chunk_size = std::cmp::max(16, notification_count / 8); // åŠ¨æ€å—å¤§å°
        
        let mut successful_count = 0;
        
        // åˆ†å—å¤„ç†ä»¥æ§åˆ¶å¹¶å‘æ•°é‡å’Œå†…å­˜ä½¿ç”¨
        // Process in chunks to control concurrency and memory usage
        for chunk in notification_data.chunks(chunk_size) {
            let chunk_futures: Vec<_> = chunk.iter()
                .map(|(entry_id, connection_id, event_type, callback_tx, event_data)| {
                    let entry_id = *entry_id;
                    let connection_id = *connection_id;
                    let event_type = event_type.clone();
                    let callback_tx = callback_tx.clone();
                    let event_data = event_data.clone();
                    
                    async move {
                        if callback_tx.send(event_data).await.is_ok() {
                            trace!(entry_id, connection_id, event_type = ?event_type, "Timer event sent");
                            1
                        } else {
                            warn!(entry_id, connection_id, "Failed to send timer event");
                            0
                        }
                    }
                })
                .collect();
            
            let chunk_results = futures::future::join_all(chunk_futures).await;
            successful_count += chunk_results.iter().sum::<usize>();
        }
        
        self.stats.processed_timers += successful_count as u64;
        
        if notification_count > 100 {
            debug!(
                total_notifications = notification_count,
                successful_notifications = successful_count,
                "Chunked concurrent timer notifications completed"
            );
        }
    }

    /// ä»é¢„æå–çš„æ•°æ®å‘é€å®šæ—¶å™¨é€šçŸ¥ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    /// Send timer notifications from pre-extracted data (backup method)
    #[allow(dead_code)]
    async fn send_timer_notifications_from_data(
        &mut self, 
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let notification_count = notification_data.len();
        
        // å¹¶å‘å‘é€æ‰€æœ‰å®šæ—¶å™¨äº‹ä»¶é€šçŸ¥
        // Concurrently send all timer event notifications
        let notification_futures: Vec<_> = notification_data
            .into_iter()
            .map(|(entry_id, connection_id, event_type, callback_tx, event_data)| {
                async move {
                    if let Err(e) = callback_tx.send(event_data).await {
                        warn!(
                            entry_id,
                            connection_id,
                            error = ?e,
                            "Failed to send timer event notification"
                        );
                        false
                    } else {
                        trace!(
                            entry_id,
                            connection_id,
                            event_type = ?event_type,
                            "Timer event notification sent successfully"
                        );
                        true
                    }
                }
            })
            .collect();

        // å¹¶å‘æ‰§è¡Œæ‰€æœ‰é€šçŸ¥å‘é€
        // Execute all notification sends concurrently
        let results = futures::future::join_all(notification_futures).await;
        
        let successful_notifications = results.iter().filter(|&&success| success).count();
        
        debug!(
            total_notifications = notification_count,
            successful_notifications,
            "Timer notifications sent after parallel processing"
        );
    }

    /// é«˜æ€§èƒ½ä¼˜åŒ–çš„æ•°æ®å¤„ç†ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
    /// High-performance optimized data processing (new version)
    async fn process_timers_from_notification_data_optimized(
        &mut self, 
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let processed_count = notification_data.len();
        
        if processed_count == 0 {
            return;
        }
        
        // ç›´æ¥è°ƒç”¨ä¼˜åŒ–çš„é€šçŸ¥å‘é€æ–¹æ³•
        // Directly call optimized notification sending method
        self.send_timer_notifications_optimized(notification_data).await;
        
        if processed_count > 1 {
            trace!(processed_count, "Optimized batch processed expired timers");
        }
    }

    /// ä»é¢„æå–çš„æ•°æ®è¿›è¡Œé¡ºåºå¤„ç†ï¼ˆfallbackï¼Œå¤‡ç”¨æ–¹æ³•ï¼‰
    /// Sequential processing from pre-extracted data (fallback, backup method)
    #[allow(dead_code)]
    async fn process_timers_from_notification_data(
        &mut self, 
        notification_data: Vec<(
            crate::timer::wheel::TimerEntryId,
            crate::timer::event::ConnectionId,
            E,
            tokio::sync::mpsc::Sender<crate::timer::event::TimerEventData<E>>,
            crate::timer::event::TimerEventData<E>,
        )>
    ) {
        let processed_count = notification_data.len();
        
        // é¡ºåºå‘é€æ‰€æœ‰å®šæ—¶å™¨äº‹ä»¶é€šçŸ¥
        // Sequentially send all timer event notifications
        for (entry_id, connection_id, event_type, callback_tx, event_data) in notification_data {
            if let Err(e) = callback_tx.send(event_data).await {
                warn!(
                    entry_id,
                    connection_id,
                    error = ?e,
                    "Failed to send timer event"
                );
            } else {
                trace!(
                    entry_id,
                    connection_id,
                    event_type = ?event_type,
                    "Timer event processed sequentially"
                );
            }
        }
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        // Update statistics
        self.stats.processed_timers += processed_count as u64;
        
        if processed_count > 1 {
            debug!(processed_count, "Sequential batch processed expired timers from fallback");
        }
    }

    /// ä¼ ç»Ÿé¡ºåºå¤„ç†å®šæ—¶å™¨ï¼ˆä½œä¸ºfallbackï¼Œå¤‡ç”¨æ–¹æ³•ï¼‰
    /// Traditional sequential timer processing (as fallback, backup method)
    #[allow(dead_code)]
    async fn process_timers_sequential(&mut self, expired_timers: Vec<TimerEntry<E>>) {
        let processed_count = expired_timers.len();
        
        // é¡ºåºè§¦å‘æ‰€æœ‰å®šæ—¶å™¨
        // Sequentially trigger all timers
        for entry in expired_timers {
            let entry_id = entry.id;
            let connection_id = entry.event.data.connection_id;
            let event_type = &entry.event.data.timeout_event;
            
            // ä½¿ç”¨ç°æœ‰çš„äº‹ä»¶è§¦å‘æœºåˆ¶
            // Use existing event trigger mechanism
            if let Err(e) = entry.event.callback_tx.send(entry.event.data.clone()).await {
                warn!(
                    entry_id,
                    connection_id,
                    error = ?e,
                    "Failed to send timer event"
                );
            } else {
                trace!(
                    entry_id,
                    connection_id,
                    event_type = ?event_type,
                    "Timer event processed sequentially"
                );
            }
        }
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        // Update statistics
        self.stats.processed_timers += processed_count as u64;
        
        if processed_count > 1 {
            debug!(processed_count, "Sequential batch processed expired timers");
        }
    }

    /// å¤„ç†å®šæ—¶å™¨ä»»åŠ¡å‘½ä»¤ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
    /// Handle timer task command (reuse existing logic)
    async fn handle_command(&mut self, command: TimerTaskCommand<E>) -> bool {
        // è¿™é‡Œå¯ä»¥å¤ç”¨ç°æœ‰çš„GlobalTimerTaskçš„handle_commandé€»è¾‘
        // We can reuse the existing GlobalTimerTask's handle_command logic here
        match command {
            TimerTaskCommand::RegisterTimer { registration, response_tx } => {
                let result = self.register_timer(registration).await;
                if let Err(err) = response_tx.send(result) {
                    warn!(error = ?err, "Failed to send register timer response");
                }
            }
            
            TimerTaskCommand::BatchRegisterTimers { batch_registration, response_tx } => {
                let result = self.batch_register_timers(batch_registration).await;
                if let Err(err) = response_tx.send(result) {
                    warn!(error = ?err, "Failed to send batch register timers response");
                }
            }
            
            TimerTaskCommand::CancelTimer { entry_id, response_tx } => {
                let result = self.cancel_timer(entry_id);
                match result {
                    Ok(cancelled) => {
                        if let Err(err) = response_tx.send(cancelled) {
                            warn!(error = ?err, "Failed to send cancel timer response");
                        }
                    }
                    Err(timer_error) => {
                        warn!(error = ?timer_error, "Failed to cancel timer");
                        let _ = response_tx.send(false); // å‘é€falseè¡¨ç¤ºå–æ¶ˆå¤±è´¥
                    }
                }
            }
            
            TimerTaskCommand::BatchCancelTimers { batch_cancellation, response_tx } => {
                let result = self.batch_cancel_timers(batch_cancellation);
                if let Err(err) = response_tx.send(result) {
                    warn!(error = ?err, "Failed to send batch cancel timers response");
                }
            }
            
            TimerTaskCommand::ClearConnectionTimers { connection_id, response_tx } => {
                let count = self.clear_connection_timers(connection_id);
                if let Err(err) = response_tx.send(count) {
                    warn!(error = ?err, "Failed to send clear timers response");
                }
            }
            
            TimerTaskCommand::GetStats { response_tx } => {
                self.update_stats();
                if let Err(err) = response_tx.send(self.stats.clone()) {
                    warn!(error = ?err, "Failed to send stats response");
                }
            }
            
            TimerTaskCommand::Shutdown => {
                info!("Received shutdown command");
                return false; // è¿”å›falseè¡¨ç¤ºåº”è¯¥å…³é—­
            }
        }
        true
    }

    // ä»¥ä¸‹æ–¹æ³•å¯ä»¥ä»ç°æœ‰çš„GlobalTimerTaskä¸­å¤åˆ¶æˆ–é€‚é…ï¼š
    // The following methods can be copied or adapted from existing GlobalTimerTask:
    
    /// æ³¨å†Œå•ä¸ªå®šæ—¶å™¨
    /// Register single timer
    async fn register_timer(&mut self, registration: TimerRegistration<E>) -> Result<TimerHandle<E>, TimerError> {
        let event_id = self.next_event_id;
        self.next_event_id += 1;

        // ä½¿ç”¨æ™ºèƒ½å·¥å‚åˆ›å»ºå®šæ—¶å™¨äº‹ä»¶ - è‡ªåŠ¨ç­–ç•¥ä¼˜åŒ–
        // Create timer event using smart factory - automatic strategy optimization
        let timeout_event_for_log = registration.timeout_event.clone();
        let timer_event = TimerEvent::from_factory(
            &self.event_factory,
            event_id,
            registration.connection_id,
            registration.timeout_event,
            registration.callback_tx,
        );

        // æ·»åŠ åˆ°æ—¶é—´è½®
        // Add to timing wheel
        let entry_id = self.timing_wheel.add_timer(registration.delay, timer_event);

        // è®°å½•è¿æ¥å…³è”
        // Record connection association
        self.connection_timers
            .entry(registration.connection_id)
            .or_default()
            .insert(entry_id);
        
        // è®°å½•åå‘æ˜ å°„
        // Record reverse mapping
        self.entry_to_connection.insert(entry_id, registration.connection_id);

        // åˆ›å»ºå®šæ—¶å™¨å¥æŸ„
        // Create timer handle
        let handle = TimerHandle::new(entry_id, self.command_tx.clone());

        trace!(
            event_id,
            entry_id,
            connection_id = registration.connection_id,
            delay_ms = registration.delay.as_millis(),
            event_type = ?timeout_event_for_log,
            "Timer registered successfully"
        );

        Ok(handle)
    }

    /// é«˜æ€§èƒ½æ‰¹é‡æ³¨å†Œå®šæ—¶å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// High-performance batch register timers (optimized version)
    async fn batch_register_timers(&mut self, batch_registration: BatchTimerRegistration<E>) -> BatchTimerResult<TimerHandle<E>> {
        let registrations = batch_registration.registrations;
        let total_count = registrations.len();
        
        if total_count == 0 {
            return BatchTimerResult::new();
        }

        let mut result = BatchTimerResult::with_capacity(total_count);
        
        // é¢„åˆ†é…event IDsï¼Œå‡å°‘åˆ†é…å¼€é”€
        // Pre-allocate event IDs to reduce allocation overhead
        let start_event_id = self.next_event_id;
        self.next_event_id += total_count as u64;
        
        // ä½¿ç”¨é¢„åˆ†é…çš„ç¼“å†²åŒºé¿å…é‡å¤åˆ†é…
        // Use pre-allocated buffers to avoid repeated allocations
        let mut timers_for_wheel = Vec::with_capacity(total_count);
        let mut registration_data = Vec::with_capacity(total_count);
        
        // é¢„åˆ†é…è¿æ¥æ˜ å°„æ›´æ–°æ•°æ®
        // Pre-allocate connection mapping update data
        self.batch_processing_buffers.connection_updates.clear();
        self.batch_processing_buffers.connection_updates.reserve(total_count / 4); // ä¼°è®¡è¿æ¥æ•°
        
        // å•æ¬¡éå†æ„å»ºæ‰€æœ‰éœ€è¦çš„æ•°æ®ç»“æ„
        // Single-pass construction of all required data structures
        let mut pool_requests = Vec::with_capacity(total_count);
        let mut callback_txs = Vec::with_capacity(total_count);
        
        for registration in &registrations {
            pool_requests.push((registration.connection_id, registration.timeout_event.clone()));
            callback_txs.push(registration.callback_tx.clone());
        }
        
        // æ‰¹é‡åˆ›å»ºå®šæ—¶å™¨äº‹ä»¶ï¼Œæ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼Œé«˜æ€§èƒ½ä¼˜åŒ–
        // Batch create timer events, smart strategy selection, high performance optimization
        let timer_events = TimerEvent::batch_from_factory(
            &self.event_factory,
            start_event_id,
            &pool_requests,
            &callback_txs,
        );
        
        // å•æ¬¡éå†æ„å»ºæ—¶é—´è½®æ•°æ®å’Œè¿æ¥æ˜ å°„é¢„å¤„ç†
        // Single-pass construction of timing wheel data and connection mapping preprocessing
        for (timer_event, registration) in timer_events.into_iter().zip(registrations.iter()) {
            timers_for_wheel.push((registration.delay, timer_event));
            registration_data.push((registration.connection_id, registration.delay));
            
            // é¢„å¤„ç†è¿æ¥æ˜ å°„æ›´æ–°
            // Preprocess connection mapping updates
            self.batch_processing_buffers.connection_updates
                .entry(registration.connection_id)
                .or_insert_with(|| Vec::with_capacity(4));
        }
        
        // ä½¿ç”¨æ—¶é—´è½®çš„æ‰¹é‡APIä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰å®šæ—¶å™¨
        // Use timing wheel's batch API to add all timers at once
        let entry_ids = self.timing_wheel.batch_add_timers(timers_for_wheel);
        
        // é«˜æ•ˆæ‰¹é‡æ›´æ–°æ˜ å°„å…³ç³»
        // Efficiently batch update mapping relationships
        for (entry_id, (connection_id, _delay)) in entry_ids.iter().zip(registration_data.iter()) {
            // ä½¿ç”¨é¢„å…ˆåˆ†é…çš„è¿æ¥æ˜ å°„ç»“æ„
            // Use pre-allocated connection mapping structure
            if let Some(connection_entries) = self.batch_processing_buffers.connection_updates.get_mut(connection_id) {
                connection_entries.push(*entry_id);
            }
            
            self.entry_to_connection.insert(*entry_id, *connection_id);
            
            // åˆ›å»ºå®šæ—¶å™¨å¥æŸ„
            // Create timer handle
            let handle = TimerHandle::new(*entry_id, self.command_tx.clone());
            result.successes.push(handle);
        }
        
        // æ‰¹é‡æ›´æ–°è¿æ¥å®šæ—¶å™¨æ˜ å°„
        // Batch update connection timer mappings
        for (connection_id, entry_ids) in self.batch_processing_buffers.connection_updates.drain() {
            let connection_timers = self.connection_timers.entry(connection_id).or_default();
            for entry_id in entry_ids {
                connection_timers.insert(entry_id);
            }
        }
        
        if total_count > 10 {
            trace!(
                batch_size = total_count,
                "High-performance batch registered timers successfully"
            );
        }
        
        result
    }

    /// å–æ¶ˆå®šæ—¶å™¨
    /// Cancel timer
    fn cancel_timer(&mut self, entry_id: TimerEntryId) -> Result<bool, TimerError> {
        let cancelled = self.timing_wheel.cancel_timer(entry_id);
        
        if cancelled {
            // ä½¿ç”¨åå‘æ˜ å°„å¿«é€Ÿå®šä½è¿æ¥å¹¶ç§»é™¤å®šæ—¶å™¨æ¡ç›®
            // Use reverse mapping to quickly locate connection and remove timer entry
            if let Some(connection_id) = self.entry_to_connection.remove(&entry_id) {
                if let Some(entry_ids) = self.connection_timers.get_mut(&connection_id) {
                    entry_ids.remove(&entry_id);
                    // å¦‚æœè¯¥è¿æ¥æ²¡æœ‰æ›´å¤šå®šæ—¶å™¨ï¼Œç§»é™¤è¿æ¥æ¡ç›®
                    // If connection has no more timers, remove connection entry
                    if entry_ids.is_empty() {
                        self.connection_timers.remove(&connection_id);
                    }
                }
            }
            
            self.stats.cancelled_timers += 1;
            trace!(entry_id, "Timer cancelled successfully");
        }
        
        Ok(cancelled)
    }

    /// æ‰¹é‡å–æ¶ˆå®šæ—¶å™¨
    /// Batch cancel timers
    fn batch_cancel_timers(&mut self, batch_cancellation: BatchTimerCancellation) -> BatchTimerResult<bool> {
        let entry_ids = batch_cancellation.entry_ids;
        let total_count = entry_ids.len();
        
        if total_count == 0 {
            return BatchTimerResult::new();
        }

        let mut result = BatchTimerResult::with_capacity(total_count);
        
        // ä½¿ç”¨æ—¶é—´è½®çš„æ‰¹é‡å–æ¶ˆAPI
        // Use timing wheel's batch cancel API
        let cancelled_count = self.timing_wheel.batch_cancel_timers(&entry_ids);
        
        // æ‰¹é‡æ”¶é›†è¦ç§»é™¤çš„æ˜ å°„ä¿¡æ¯
        // Batch collect mapping info to remove
        let mut connections_to_update: HashMap<ConnectionId, Vec<TimerEntryId>> = HashMap::new();
        
        for (index, entry_id) in entry_ids.iter().enumerate() {
            // æ£€æŸ¥æ˜¯å¦åœ¨entry_to_connectionæ˜ å°„ä¸­ï¼ˆè¡¨ç¤ºæˆåŠŸå–æ¶ˆï¼‰
            // Check if exists in entry_to_connection mapping (indicates successful cancellation)
            if let Some(connection_id) = self.entry_to_connection.remove(entry_id) {
                // æ”¶é›†è¿æ¥æ˜ å°„ä¿¡æ¯ï¼Œæ‰¹é‡å¤„ç†
                // Collect connection mapping info for batch processing
                connections_to_update
                    .entry(connection_id)
                    .or_default()
                    .push(*entry_id);
                
                self.stats.cancelled_timers += 1;
                result.successes.push(true);
            } else {
                result.failures.push((index, TimerError::TimerNotFound));
            }
        }
        
        // æ‰¹é‡æ›´æ–°è¿æ¥å®šæ—¶å™¨æ˜ å°„
        // Batch update connection timer mappings  
        for (connection_id, cancelled_entry_ids) in connections_to_update {
            if let Some(entry_ids_set) = self.connection_timers.get_mut(&connection_id) {
                for entry_id in cancelled_entry_ids {
                    entry_ids_set.remove(&entry_id);
                }
                
                // å¦‚æœè¯¥è¿æ¥æ²¡æœ‰æ›´å¤šå®šæ—¶å™¨ï¼Œç§»é™¤è¿æ¥æ¡ç›®
                // If connection has no more timers, remove connection entry
                if entry_ids_set.is_empty() {
                    self.connection_timers.remove(&connection_id);
                }
            }
        }
        
        trace!(
            batch_size = total_count,
            cancelled_count = result.success_count(),
            wheel_cancelled_count = cancelled_count,
            "Batch cancelled timers using wheel batch API"
        );
        
        result
    }

    /// æ¸…ç†è¿æ¥çš„æ‰€æœ‰å®šæ—¶å™¨
    /// Clear all timers for connection
    fn clear_connection_timers(&mut self, connection_id: ConnectionId) -> usize {
        let mut count = 0;
        
        if let Some(entry_ids) = self.connection_timers.remove(&connection_id) {
            for entry_id in &entry_ids {
                if self.timing_wheel.cancel_timer(*entry_id) {
                    // ä»åå‘æ˜ å°„ä¸­ç§»é™¤
                    // Remove from reverse mapping
                    self.entry_to_connection.remove(entry_id);
                    count += 1;
                    self.stats.cancelled_timers += 1;
                }
            }
            
            debug!(
                connection_id,
                count,
                "Cleared all timers for connection"
            );
        }
        
        count
    }

    /// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    /// Update statistics
    fn update_stats(&mut self) {
        self.stats.total_timers = self.timing_wheel.timer_count();
        self.stats.active_connections = self.connection_timers.len();
        self.stats.wheel_stats = self.timing_wheel.stats();
    }

    /// è®¡ç®—åŠ¨æ€æ¨è¿›é—´éš”
    /// Calculate dynamic advance interval
    fn calculate_advance_interval(&self) -> tokio::time::Interval {
        let timer_count = self.timing_wheel.timer_count();
        
        let interval_ms = if timer_count < 100 {
            100
        } else if timer_count < 1000 {
            50
        } else if timer_count < 5000 {
            20
        } else {
            10
        };
        
        interval(Duration::from_millis(interval_ms))
    }

    /// è·å–ä¸‹æ¬¡å”¤é†’æ—¶é—´
    /// Get next wakeup time
    fn get_next_wakeup_time(&mut self) -> Instant {
        match self.timing_wheel.next_expiry_time() {
            Some(expiry) => {
                let now = Instant::now();
                if expiry <= now + Duration::from_millis(5) {
                    now + Duration::from_millis(5)
                } else {
                    expiry
                }
            }
            None => Instant::now() + Duration::from_secs(1),
        }
    }

    /// è·å–å¹¶è¡Œå¤„ç†ç³»ç»Ÿçš„æ€§èƒ½ç»Ÿè®¡
    /// Get parallel processing system performance statistics
    pub fn get_parallel_stats(&self) -> &crate::timer::parallel::types::ParallelProcessingStats {
        self.parallel_system.get_stats()
    }
}

/// æ··åˆå¹¶è¡Œå®šæ—¶å™¨ä»»åŠ¡å¥æŸ„
/// Hybrid parallel timer task handle  
#[derive(Clone)]
pub struct HybridTimerTaskHandle<E: EventDataTrait> {
    command_tx: mpsc::Sender<TimerTaskCommand<E>>,
}

impl<E: EventDataTrait> HybridTimerTaskHandle<E> {
    /// åˆ›å»ºæ–°çš„å¥æŸ„
    /// Create new handle
    pub fn new(command_tx: mpsc::Sender<TimerTaskCommand<E>>) -> Self {
        Self { command_tx }
    }

    /// è·å–å‘½ä»¤å‘é€é€šé“çš„å…‹éš†
    /// Get clone of command sender channel
    pub fn command_sender(&self) -> mpsc::Sender<TimerTaskCommand<E>> {
        self.command_tx.clone()
    }
    
    /// æ³¨å†Œå®šæ—¶å™¨
    /// Register timer
    pub async fn register_timer(
        &self,
        registration: TimerRegistration<E>,
    ) -> Result<TimerHandle<E>, TimerError> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        let command = TimerTaskCommand::RegisterTimer {
            registration,
            response_tx,
        };

        self.command_tx
            .send(command)
            .await
            .map_err(|_| TimerError::TaskShutdown)?;

        response_rx
            .await
            .map_err(|_| TimerError::TaskShutdown)?
    }

    /// æ‰¹é‡æ³¨å†Œå®šæ—¶å™¨ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
    /// Batch register timers (high-performance version)
    pub async fn batch_register_timers(
        &self,
        batch_registration: BatchTimerRegistration<E>,
    ) -> Result<BatchTimerResult<TimerHandle<E>>, TimerError> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        let command = TimerTaskCommand::BatchRegisterTimers {
            batch_registration,
            response_tx,
        };

        self.command_tx
            .send(command)
            .await
            .map_err(|_| TimerError::TaskShutdown)?;

        response_rx
            .await
            .map_err(|_| TimerError::TaskShutdown)
    }

    /// æ‰¹é‡å–æ¶ˆå®šæ—¶å™¨ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
    /// Batch cancel timers (high-performance version)
    pub async fn batch_cancel_timers(
        &self,
        batch_cancellation: BatchTimerCancellation,
    ) -> Result<BatchTimerResult<bool>, TimerError> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        let command = TimerTaskCommand::BatchCancelTimers {
            batch_cancellation,
            response_tx,
        };

        self.command_tx
            .send(command)
            .await
            .map_err(|_| TimerError::TaskShutdown)?;

        response_rx
            .await
            .map_err(|_| TimerError::TaskShutdown)
    }

    /// æ¸…é™¤è¿æ¥çš„æ‰€æœ‰å®šæ—¶å™¨
    /// Clear all timers for a connection
    pub async fn clear_connection_timers(&self, connection_id: ConnectionId) -> Result<usize, TimerError> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        let command = TimerTaskCommand::ClearConnectionTimers {
            connection_id,
            response_tx,
        };

        self.command_tx
            .send(command)
            .await
            .map_err(|_| TimerError::TaskShutdown)?;

        response_rx
            .await
            .map_err(|_| TimerError::TaskShutdown)
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    /// Get statistics
    pub async fn get_stats(&self) -> Result<TimerTaskStats, TimerError> {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();
        
        let command = TimerTaskCommand::GetStats { response_tx };

        self.command_tx
            .send(command)
            .await
            .map_err(|_| TimerError::TaskShutdown)?;

        response_rx
            .await
            .map_err(|_| TimerError::TaskShutdown)
    }

    /// å…³é—­å®šæ—¶å™¨ä»»åŠ¡
    /// Shutdown timer task
    pub async fn shutdown(&self) -> Result<(), TimerError> {
        self.command_tx
            .send(TimerTaskCommand::Shutdown)
            .await
            .map_err(|_| TimerError::TaskShutdown)
    }
}

/// å¯åŠ¨æ··åˆå¹¶è¡Œå®šæ—¶å™¨ä»»åŠ¡
/// Start hybrid parallel timer task
pub fn start_hybrid_timer_task<E: EventDataTrait>() -> HybridTimerTaskHandle<E> {
    let (task, command_tx) = HybridTimerTask::new_default();
    let handle = HybridTimerTaskHandle::new(command_tx.clone());
    
    tokio::spawn(async move {
        task.run().await;
    });
    
    info!("Hybrid parallel timer task started");
    handle
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::timer::event::traits::EventDataTrait;
    use crate::timer::task::types::{TimerRegistration, BatchTimerRegistration, BatchTimerCancellation};
    use crate::timer::wheel::TimerEntry;
    use std::time::Duration;
    use tokio::time::Instant;

    /// æµ‹è¯•ç”¨çš„è¶…æ—¶äº‹ä»¶ç±»å‹
    /// Test timeout event type
    #[derive(Debug, Clone, Default, PartialEq)]
    struct TestTimeoutEvent {
        pub event_type: String,
        pub payload: Vec<u8>,
    }

    /// åˆ›å»ºæµ‹è¯•ç”¨çš„å®šæ—¶å™¨æ¡ç›®
    /// Create test timer entries
    #[allow(unused)]
    fn create_test_timer_entries<E: EventDataTrait>(count: usize) -> Vec<TimerEntry<E>> {
        let (tx, _rx) = tokio::sync::mpsc::channel(1);
        let mut entries = Vec::with_capacity(count);
        let factory = crate::timer::event::traits::EventFactory::<E>::new();
        
        for i in 0..count {
            let timer_event = TimerEvent::from_factory(
                &factory,
                i as u64, // id
                (i % 10000) as u32, // connection_id
                E::default(),
                tx.clone(),
            );
            
            entries.push(TimerEntry {
                id: i as u64,
                expiry_time: tokio::time::Instant::now() + tokio::time::Duration::from_millis(1000),
                event: timer_event,
            });
        }
        
        entries
    }

    /// åˆ›å»ºæµ‹è¯•ç”¨çš„å®šæ—¶å™¨æ³¨å†Œ
    /// Create test timer registrations
    fn create_test_timer_registrations<E: EventDataTrait>(count: usize) -> Vec<TimerRegistration<E>> {
        let (tx, _rx) = tokio::sync::mpsc::channel(count);
        let mut registrations = Vec::with_capacity(count);
        
        for i in 0..count {
            registrations.push(TimerRegistration {
                connection_id: (i % 1000) as u32,
                delay: Duration::from_millis(100 + (i % 1000) as u64),
                timeout_event: E::default(),
                callback_tx: tx.clone(),
            });
        }
        
        registrations
    }

    #[tokio::test]
    async fn test_hybrid_timer_basic_functionality() {
        println!("\nğŸ·ï¸  åŸºæœ¬åŠŸèƒ½æµ‹è¯•");
        println!("================");
        
        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new_default();
        let handle = HybridTimerTaskHandle::new(command_tx);
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // æµ‹è¯•å•ä¸ªå®šæ—¶å™¨æ³¨å†Œ
        let (callback_tx, mut callback_rx) = tokio::sync::mpsc::channel(1);
        let registration = TimerRegistration {
            connection_id: 1,
            delay: Duration::from_millis(50),
            timeout_event: TestTimeoutEvent {
                event_type: "test".to_string(),
                payload: vec![1, 2, 3],
            },
            callback_tx,
        };
        
        let _timer_handle = handle.register_timer(registration).await.expect("Failed to register timer");
        
        // ç­‰å¾…å®šæ—¶å™¨è§¦å‘
        tokio::time::timeout(Duration::from_secs(1), callback_rx.recv())
            .await
            .expect("Timer should fire within timeout")
            .expect("Should receive timer event");
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡");
    }

    #[tokio::test]
    async fn test_hybrid_timer_batch_operations() {
        println!("\nğŸ“¦ æ‰¹é‡æ“ä½œæµ‹è¯•");
        println!("================");
        
        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new_default();
        let handle = HybridTimerTaskHandle::new(command_tx);
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // æµ‹è¯•æ‰¹é‡æ³¨å†Œ
        let registrations = create_test_timer_registrations(100);
        let batch_registration = BatchTimerRegistration { registrations };
        
        let start_time = Instant::now();
        let batch_result = handle.batch_register_timers(batch_registration).await.expect("Batch registration failed");
        let registration_time = start_time.elapsed();
        
        println!("æ‰¹é‡æ³¨å†Œ100ä¸ªå®šæ—¶å™¨è€—æ—¶: {:.3}ms", registration_time.as_secs_f64() * 1000.0);
        println!("æˆåŠŸæ³¨å†Œ: {}", batch_result.success_count());
        println!("å¤±è´¥æ³¨å†Œ: {}", batch_result.failure_count());
        
        assert_eq!(batch_result.success_count(), 100);
        
        // æµ‹è¯•æ‰¹é‡å–æ¶ˆ
        let entry_ids: Vec<_> = batch_result.successes.iter().map(|h| h.entry_id).collect();
        let batch_cancellation = BatchTimerCancellation { entry_ids };
        
        let start_time = Instant::now();
        let cancel_result = handle.batch_cancel_timers(batch_cancellation).await.expect("Batch cancellation failed");
        let cancellation_time = start_time.elapsed();
        
        println!("æ‰¹é‡å–æ¶ˆ100ä¸ªå®šæ—¶å™¨è€—æ—¶: {:.3}ms", cancellation_time.as_secs_f64() * 1000.0);
        println!("æˆåŠŸå–æ¶ˆ: {}", cancel_result.success_count());
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("âœ… æ‰¹é‡æ“ä½œæµ‹è¯•é€šè¿‡");
    }

    #[tokio::test]
    async fn test_hybrid_timer_connection_management() {
        println!("\nğŸ”— è¿æ¥ç®¡ç†æµ‹è¯•");
        println!("================");
        
        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new_default();
        let handle = HybridTimerTaskHandle::new(command_tx);
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // ä¸ºå¤šä¸ªè¿æ¥æ³¨å†Œå®šæ—¶å™¨
        let mut handles_by_connection = std::collections::HashMap::new();
        
        for conn_id in 1..=10 {
            let mut connection_handles = Vec::new();
            
            for i in 0..10 {
                let (callback_tx, _callback_rx) = tokio::sync::mpsc::channel(1);
                let registration = TimerRegistration {
                    connection_id: conn_id,
                    delay: Duration::from_secs(10), // é•¿æ—¶é—´ï¼Œé¿å…è‡ªåŠ¨è¿‡æœŸ
                    timeout_event: TestTimeoutEvent {
                        event_type: format!("conn_{}_timer_{}", conn_id, i),
                        payload: vec![conn_id as u8, i as u8],
                    },
                    callback_tx,
                };
                
                let timer_handle = handle.register_timer(registration).await.expect("Failed to register timer");
                connection_handles.push(timer_handle);
            }
            
            handles_by_connection.insert(conn_id, connection_handles);
        }
        
        // æµ‹è¯•è·å–ç»Ÿè®¡ä¿¡æ¯
        let stats = handle.get_stats().await.expect("Failed to get stats");
        println!("æ€»å®šæ—¶å™¨æ•°: {}", stats.total_timers);
        println!("æ´»è·ƒè¿æ¥æ•°: {}", stats.active_connections);
        assert_eq!(stats.active_connections, 10);
        
        // æµ‹è¯•æ¸…ç†ç‰¹å®šè¿æ¥çš„å®šæ—¶å™¨
        let cleared_count = handle.clear_connection_timers(5).await.expect("Failed to clear connection timers");
        println!("è¿æ¥5æ¸…ç†çš„å®šæ—¶å™¨æ•°: {}", cleared_count);
        assert_eq!(cleared_count, 10);
        
        // éªŒè¯ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
        let stats_after_clear = handle.get_stats().await.expect("Failed to get updated stats");
        println!("æ¸…ç†åæ´»è·ƒè¿æ¥æ•°: {}", stats_after_clear.active_connections);
        assert_eq!(stats_after_clear.active_connections, 9);
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("âœ… è¿æ¥ç®¡ç†æµ‹è¯•é€šè¿‡");
    }

    #[tokio::test]
    #[ignore] // æ€§èƒ½æµ‹è¯•ï¼Œéœ€è¦å•ç‹¬è¿è¡Œ: cargo test test_comprehensive_hybrid_benchmark -- --ignored
    async fn test_comprehensive_hybrid_benchmark() {
        // ç­‰å¾…ç³»ç»Ÿç¨³å®š
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        println!("\nğŸ† æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•");
        println!("=========================================");
        println!("å¯¹æ¯”ä¸åŒæ‰¹é‡å¤§å°ä¸‹çš„æ··åˆå¤„ç†ç³»ç»Ÿæ€§èƒ½");
        println!();

        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new(2048, 32);
        let handle = HybridTimerTaskHandle::new(command_tx.clone());
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // æµ‹è¯•åœºæ™¯ï¼šä¸åŒæ‰¹é‡å¤§å°ä¸‹çš„æ€§èƒ½å¯¹æ¯”
        let benchmark_cases = vec![
            (1, "å•ä¸ªå®šæ—¶å™¨"),
            (8, "è¶…å°æ‰¹é‡"),
            (16, "å°æ‰¹é‡-1"),
            (32, "å°æ‰¹é‡-2 (é˜ˆå€¼)"),
            (64, "ä¸­æ‰¹é‡-1"),
            (128, "ä¸­æ‰¹é‡-2"),
            (256, "å¤§æ‰¹é‡-1"),
            (512, "å¤§æ‰¹é‡-2"),
            (1024, "è¶…å¤§æ‰¹é‡-1"),
            (2048, "è¶…å¤§æ‰¹é‡-2"),
            (4096, "å·¨å¤§æ‰¹é‡"),
        ];

        for (batch_size, scenario) in benchmark_cases {
            println!("ğŸ {} ({} ä¸ªå®šæ—¶å™¨):", scenario, batch_size);
            
            // é¢„çƒ­é˜¶æ®µ
            for _ in 0..5 {
                let warmup_registrations = create_test_timer_registrations(batch_size);
                let warmup_batch = BatchTimerRegistration { registrations: warmup_registrations };
                let _ = handle.batch_register_timers(warmup_batch).await;
            }
            
            // åŸºå‡†æµ‹è¯•
            let benchmark_iterations = if batch_size <= 32 { 1000 } else if batch_size <= 512 { 500 } else { 100 };
            let mut registration_durations = Vec::with_capacity(benchmark_iterations);
            let mut cancellation_durations = Vec::with_capacity(benchmark_iterations);
            
            let benchmark_start = Instant::now();
            
            for _ in 0..benchmark_iterations {
                // æµ‹è¯•æ‰¹é‡æ³¨å†Œæ€§èƒ½
                let registrations = create_test_timer_registrations(batch_size);
                let batch_registration = BatchTimerRegistration { registrations };
                
                let reg_start = Instant::now();
                let batch_result = handle.batch_register_timers(batch_registration).await
                    .expect("Batch registration failed");
                registration_durations.push(reg_start.elapsed());
                
                // æµ‹è¯•æ‰¹é‡å–æ¶ˆæ€§èƒ½
                let entry_ids: Vec<_> = batch_result.successes.iter().map(|h| h.entry_id).collect();
                let batch_cancellation = BatchTimerCancellation { entry_ids };
                
                let cancel_start = Instant::now();
                let _ = handle.batch_cancel_timers(batch_cancellation).await
                    .expect("Batch cancellation failed");
                cancellation_durations.push(cancel_start.elapsed());
            }
            
            let total_benchmark_time = benchmark_start.elapsed();
            
            // è®¡ç®—ç»Ÿè®¡æ•°æ®
            let avg_reg_duration = registration_durations.iter().sum::<Duration>() / registration_durations.len() as u32;
            let min_reg_duration = *registration_durations.iter().min().unwrap();
            let max_reg_duration = *registration_durations.iter().max().unwrap();
            
            let avg_cancel_duration = cancellation_durations.iter().sum::<Duration>() / cancellation_durations.len() as u32;
            let min_cancel_duration = *cancellation_durations.iter().min().unwrap();
            let max_cancel_duration = *cancellation_durations.iter().max().unwrap();
            
            let reg_nanos_per_op = if batch_size > 0 {
                avg_reg_duration.as_nanos() / batch_size as u128
            } else {
                0
            };
            
            let cancel_nanos_per_op = if batch_size > 0 {
                avg_cancel_duration.as_nanos() / batch_size as u128
            } else {
                0
            };
            
            let total_ops = batch_size as f64 * benchmark_iterations as f64 * 2.0; // æ³¨å†Œ + å–æ¶ˆ
            let throughput = if total_benchmark_time.as_secs_f64() > 0.0 {
                total_ops / total_benchmark_time.as_secs_f64()
            } else {
                0.0
            };
            
            println!("  ğŸ“ˆ æ³¨å†Œæ€§èƒ½æŒ‡æ ‡:");
            println!("    å¹³å‡å»¶è¿Ÿ: {:.3}Âµs", avg_reg_duration.as_secs_f64() * 1_000_000.0);
            println!("    æœ€å°å»¶è¿Ÿ: {:.3}Âµs", min_reg_duration.as_secs_f64() * 1_000_000.0);
            println!("    æœ€å¤§å»¶è¿Ÿ: {:.3}Âµs", max_reg_duration.as_secs_f64() * 1_000_000.0);
            println!("    æ¯æ“ä½œ: {} çº³ç§’", reg_nanos_per_op);
            println!();
            
            println!("  ğŸ“‰ å–æ¶ˆæ€§èƒ½æŒ‡æ ‡:");
            println!("    å¹³å‡å»¶è¿Ÿ: {:.3}Âµs", avg_cancel_duration.as_secs_f64() * 1_000_000.0);
            println!("    æœ€å°å»¶è¿Ÿ: {:.3}Âµs", min_cancel_duration.as_secs_f64() * 1_000_000.0);
            println!("    æœ€å¤§å»¶è¿Ÿ: {:.3}Âµs", max_cancel_duration.as_secs_f64() * 1_000_000.0);
            println!("    æ¯æ“ä½œ: {} çº³ç§’", cancel_nanos_per_op);
            println!();
            
            println!("  ğŸ¯ æ•´ä½“æ€§èƒ½:");
            println!("    æ€»ååé‡: {:.0} ops/sec", throughput);
            println!("    æµ‹è¯•è¿­ä»£: {} æ¬¡", benchmark_iterations);
            println!();
            
            // æ€§èƒ½ç­‰çº§è¯„ä¼°
            let overall_nanos_per_op = (reg_nanos_per_op + cancel_nanos_per_op) / 2;
            let performance_grade = match overall_nanos_per_op {
                0..=100 => "Sçº§ (å“è¶Š)",
                101..=200 => "Açº§ (ä¼˜ç§€)",
                201..=400 => "Bçº§ (è‰¯å¥½)",
                401..=800 => "Cçº§ (ä¸€èˆ¬)",
                _ => "Dçº§ (éœ€æ”¹è¿›)",
            };
            
            println!("  ğŸ† æ€§èƒ½ç­‰çº§: {}", performance_grade);
            
            // é¢„æœŸæ¨¡å¼æç¤º
            if batch_size >= 32 {
                println!("  ğŸ”„ é¢„æœŸä½¿ç”¨: æ··åˆå¹¶è¡Œå¤„ç†");
            } else {
                println!("  âš¡ é¢„æœŸä½¿ç”¨: é¡ºåºå¤„ç†");
            }
            
            println!();
        }

        // è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        let final_stats = handle.get_stats().await.expect("Failed to get final stats");
        println!("ğŸ¯ æœ€ç»ˆç³»ç»Ÿç»Ÿè®¡:");
        println!("  æ€»å¤„ç†å®šæ—¶å™¨: {}", final_stats.processed_timers);
        println!("  å–æ¶ˆå®šæ—¶å™¨: {}", final_stats.cancelled_timers);
        println!("  æ´»è·ƒè¿æ¥: {}", final_stats.active_connections);
        println!("  æ€»å®šæ—¶å™¨: {}", final_stats.total_timers);
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("\nâœ… ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ");
    }

    #[tokio::test]
    #[ignore] // å‹åŠ›æµ‹è¯•ï¼Œéœ€è¦å•ç‹¬è¿è¡Œ
    async fn test_hybrid_timer_stress_test() {
        println!("\nğŸ’ª æ··åˆå®šæ—¶å™¨ç³»ç»Ÿå‹åŠ›æµ‹è¯•");
        println!("=========================");
        
        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new(4096, 64);
        let handle = HybridTimerTaskHandle::new(command_tx);
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // å¹¶å‘æ³¨å†Œå¤§é‡å®šæ—¶å™¨
        let concurrent_tasks = 10;
        let timers_per_task = 1000;
        let total_timers = concurrent_tasks * timers_per_task;
        
        println!("å¯åŠ¨ {} ä¸ªå¹¶å‘ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å¤„ç† {} ä¸ªå®šæ—¶å™¨", concurrent_tasks, timers_per_task);
        println!("æ€»å®šæ—¶å™¨æ•°: {}", total_timers);
        
        let stress_start = Instant::now();
        let mut handles = Vec::new();
        
        for task_id in 0..concurrent_tasks {
            let handle_clone = handle.clone();
            let task_handle = tokio::spawn(async move {
                let mut task_timers = Vec::new();
                
                // æ‰¹é‡æ³¨å†Œ
                for batch_start in (0..timers_per_task).step_by(100) {
                    let batch_end = std::cmp::min(batch_start + 100, timers_per_task);
                    let batch_size = batch_end - batch_start;
                    
                    let registrations = create_test_timer_registrations(batch_size);
                    let batch_registration = BatchTimerRegistration { registrations };
                    
                    match handle_clone.batch_register_timers(batch_registration).await {
                        Ok(batch_result) => {
                            task_timers.extend(batch_result.successes);
                        }
                        Err(e) => {
                            eprintln!("Task {} batch registration failed: {:?}", task_id, e);
                            return 0;
                        }
                    }
                }
                
                // æ‰¹é‡å–æ¶ˆä¸€åŠå®šæ—¶å™¨
                let half_count = task_timers.len() / 2;
                if half_count > 0 {
                    let entry_ids: Vec<_> = task_timers[..half_count].iter().map(|h| h.entry_id).collect();
                    let batch_cancellation = BatchTimerCancellation { entry_ids };
                    
                    if let Err(e) = handle_clone.batch_cancel_timers(batch_cancellation).await {
                        eprintln!("Task {} batch cancellation failed: {:?}", task_id, e);
                    }
                }
                
                task_timers.len()
            });
            
            handles.push(task_handle);
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        let mut total_processed = 0;
        for handle in handles {
            match handle.await {
                Ok(processed) => total_processed += processed,
                Err(e) => eprintln!("Task failed: {:?}", e),
            }
        }
        
        let stress_duration = stress_start.elapsed();
        
        println!("å‹åŠ›æµ‹è¯•å®Œæˆï¼");
        println!("æ€»å¤„ç†æ—¶é—´: {:.3}s", stress_duration.as_secs_f64());
        println!("æ€»å¤„ç†å®šæ—¶å™¨: {}", total_processed);
        println!("å¹³å‡ååé‡: {:.0} timers/sec", total_processed as f64 / stress_duration.as_secs_f64());
        
        // è·å–æœ€ç»ˆç»Ÿè®¡
        let final_stats = handle.get_stats().await.expect("Failed to get final stats");
        println!("ç³»ç»Ÿæœ€ç»ˆçŠ¶æ€:");
        println!("  æ´»è·ƒå®šæ—¶å™¨: {}", final_stats.total_timers);
        println!("  æ´»è·ƒè¿æ¥: {}", final_stats.active_connections);
        println!("  å·²å¤„ç†å®šæ—¶å™¨: {}", final_stats.processed_timers);
        println!("  å·²å–æ¶ˆå®šæ—¶å™¨: {}", final_stats.cancelled_timers);
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("âœ… å‹åŠ›æµ‹è¯•é€šè¿‡");
    }

    #[tokio::test]
    async fn test_hybrid_timer_memory_efficiency() {
        println!("\nğŸ§  å†…å­˜æ•ˆç‡æµ‹è¯•");
        println!("================");
        
        let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new_default();
        let handle = HybridTimerTaskHandle::new(command_tx);
        
        // å¯åŠ¨ä»»åŠ¡åœ¨åå°
        let task_handle = tokio::spawn(async move {
            task.run().await;
        });
        
        // æµ‹è¯•å¤§æ‰¹é‡æ³¨å†Œæ—¶çš„å†…å­˜ä½¿ç”¨
        let large_batch_size = 10000;
        println!("æµ‹è¯•å¤§æ‰¹é‡ ({}) å®šæ—¶å™¨æ³¨å†Œçš„å†…å­˜æ•ˆç‡", large_batch_size);
        
        let registrations = create_test_timer_registrations(large_batch_size);
        let batch_registration = BatchTimerRegistration { registrations };
        
        let memory_start = std::alloc::System.used_memory();
        let reg_start = Instant::now();
        
        let batch_result = handle.batch_register_timers(batch_registration).await
            .expect("Large batch registration failed");
        
        let reg_duration = reg_start.elapsed();
        let memory_after_reg = std::alloc::System.used_memory();
        
        println!("æ³¨å†Œå®Œæˆè€—æ—¶: {:.3}ms", reg_duration.as_secs_f64() * 1000.0);
        println!("å†…å­˜å¢é•¿: {} bytes", memory_after_reg.saturating_sub(memory_start));
        println!("æˆåŠŸæ³¨å†Œ: {}", batch_result.success_count());
        
        // æµ‹è¯•æ‰¹é‡å–æ¶ˆ
        let entry_ids: Vec<_> = batch_result.successes.iter().map(|h| h.entry_id).collect();
        let batch_cancellation = BatchTimerCancellation { entry_ids };
        
        let cancel_start = Instant::now();
        let cancel_result = handle.batch_cancel_timers(batch_cancellation).await
            .expect("Large batch cancellation failed");
        let cancel_duration = cancel_start.elapsed();
        let memory_after_cancel = std::alloc::System.used_memory();
        
        println!("å–æ¶ˆå®Œæˆè€—æ—¶: {:.3}ms", cancel_duration.as_secs_f64() * 1000.0);
        println!("å–æ¶ˆåå†…å­˜: {} bytes (ç›¸å¯¹äºæ³¨å†Œå: {})", 
                 memory_after_cancel, 
                 memory_after_cancel as i64 - memory_after_reg as i64);
        println!("æˆåŠŸå–æ¶ˆ: {}", cancel_result.success_count());
        
        // å…³é—­ä»»åŠ¡
        handle.shutdown().await.expect("Failed to shutdown");
        task_handle.await.expect("Task should complete");
        
        println!("âœ… å†…å­˜æ•ˆç‡æµ‹è¯•é€šè¿‡");
    }

    // TODO: æ·»åŠ ä¸€ä¸ªè‡ªå®šä¹‰å†…å­˜åˆ†é…å™¨è¿½è¸ªå™¨æ¥æ›´å‡†ç¡®åœ°æµ‹é‡å†…å­˜ä½¿ç”¨
    trait MemoryUsage {
        fn used_memory(&self) -> usize;
    }
    
    impl MemoryUsage for std::alloc::System {
        fn used_memory(&self) -> usize {
            // è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦å®ç°ï¼Œåœ¨å®é™…ç¯å¢ƒä¸­åº”è¯¥ä½¿ç”¨çœŸå®çš„å†…å­˜è¿½è¸ª
            // This is a placeholder implementation, should use real memory tracking in production
            0
        }
    }

    #[tokio::test]
    async fn test_hybrid_timer_threshold_behavior() {
        println!("\nğŸ¯ å¹¶è¡Œé˜ˆå€¼è¡Œä¸ºæµ‹è¯•");
        println!("===================");
        
        // æµ‹è¯•ä¸åŒå¹¶è¡Œé˜ˆå€¼çš„è¡Œä¸º
        let threshold_tests = vec![
            (16, "ä½é˜ˆå€¼"),
            (32, "æ ‡å‡†é˜ˆå€¼"),
            (64, "é«˜é˜ˆå€¼"),
            (128, "è¶…é«˜é˜ˆå€¼"),
        ];
        
        for (threshold, name) in threshold_tests {
            println!("æµ‹è¯• {} (é˜ˆå€¼: {})", name, threshold);
            
            let (task, command_tx) = HybridTimerTask::<TestTimeoutEvent>::new(1024, threshold);
            let handle = HybridTimerTaskHandle::new(command_tx);
            
            // å¯åŠ¨ä»»åŠ¡
            let task_handle = tokio::spawn(async move {
                task.run().await;
            });
            
            // æµ‹è¯•é˜ˆå€¼å‰åçš„æ€§èƒ½å·®å¼‚
            let test_sizes = vec![threshold / 2, threshold - 1, threshold, threshold + 1, threshold * 2];
            
            for batch_size in test_sizes {
                let registrations = create_test_timer_registrations(batch_size);
                let batch_registration = BatchTimerRegistration { registrations };
                
                let start = Instant::now();
                let result = handle.batch_register_timers(batch_registration).await
                    .expect("Registration failed");
                let duration = start.elapsed();
                
                let mode = if batch_size >= threshold { "å¹¶è¡Œ" } else { "é¡ºåº" };
                println!("  æ‰¹é‡å¤§å° {}: {:.3}Âµs ({}æ¨¡å¼)", 
                         batch_size, 
                         duration.as_secs_f64() * 1_000_000.0,
                         mode);
                
                // æ¸…ç†
                let entry_ids: Vec<_> = result.successes.iter().map(|h| h.entry_id).collect();
                let batch_cancellation = BatchTimerCancellation { entry_ids };
                let _ = handle.batch_cancel_timers(batch_cancellation).await;
            }
            
            // å…³é—­ä»»åŠ¡
            handle.shutdown().await.expect("Failed to shutdown");
            task_handle.await.expect("Task should complete");
            
            println!();
        }
        
        println!("âœ… å¹¶è¡Œé˜ˆå€¼è¡Œä¸ºæµ‹è¯•é€šè¿‡");
    }
}