//! æ··åˆå¹¶è¡Œå®šæ—¶å™¨å¤„ç†æ¨¡å— 
//! Hybrid Parallel Timer Processing Module
//!
//! è¯¥æ¨¡å—å®ç°äº†ä¸‰å±‚å¹¶è¡Œä¼˜åŒ–æ¶æ„ï¼š
//! 1. SIMDå‘é‡åŒ– - å•çº¿ç¨‹å†…å¹¶è¡Œå¤„ç†
//! 2. Rayonæ•°æ®å¹¶è¡Œ - CPUå¯†é›†å‹æ‰¹é‡è®¡ç®—
//! 3. tokioå¼‚æ­¥å¹¶å‘ - I/Oå¯†é›†å‹äº‹ä»¶åˆ†å‘
//! 4. é›¶æ‹·è´é€šé“ - åŸºäºå¼•ç”¨ä¼ é€’é¿å…æ•°æ®å…‹éš†
//! 5. å•çº¿ç¨‹ç›´é€š - ç»•è¿‡å¼‚æ­¥è°ƒåº¦çš„ç›´æ¥è·¯å¾„
//!
//! This module implements a three-tier parallel optimization architecture:
//! 1. SIMD Vectorization - Intra-thread parallel processing
//! 2. Rayon Data Parallelism - CPU-intensive batch computation  
//! 3. tokio Async Concurrency - I/O-intensive event dispatching
//! 4. Zero-Copy Channels - Reference passing to avoid data cloning
//! 5. Single-Thread Bypass - Direct path bypassing async scheduling

use crate::timer::event::{TimerEventData, ConnectionId, zero_copy::{ZeroCopyEventDelivery}};
use crate::timer::wheel::{TimerEntry, TimerEntryId};
use rayon::prelude::*;
use std::sync::{Arc, Mutex};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::time::{Duration, Instant};
use tokio::sync::mpsc;
use wide::{u32x8, u64x4};
use crate::core::endpoint::timing::TimeoutEvent;
use tracing;

/// å•çº¿ç¨‹ç›´é€šä¼˜åŒ–æ¨¡å— - ç»•è¿‡å¼‚æ­¥è°ƒåº¦çš„åŒæ­¥è·¯å¾„
/// Single-thread bypass optimization module - synchronous path bypassing async scheduling
pub mod single_thread_bypass {
    use super::*;
    
    /// å•çº¿ç¨‹æ‰§è¡Œæ¨¡å¼åˆ¤æ–­
    /// Single-thread execution mode detection
    #[derive(Debug, Clone, Copy)]
    pub enum ExecutionMode {
        /// å•çº¿ç¨‹ç›´æ¥æ‰§è¡Œï¼ˆé›¶å¼‚æ­¥å¼€é”€ï¼‰
        /// Single-thread direct execution (zero async overhead)
        SingleThreadDirect,
        /// å•çº¿ç¨‹ä½†å¼‚æ­¥è°ƒåº¦
        /// Single-thread with async scheduling  
        SingleThreadAsync,
        /// å¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ
        /// Multi-thread parallel execution
        MultiThreadParallel,
    }
    
    /// æ™ºèƒ½æ‰§è¡Œæ¨¡å¼é€‰æ‹©å™¨
    /// Smart execution mode selector
    pub struct ExecutionModeSelector {
        /// å½“å‰çº¿ç¨‹æ•°
        /// Current thread count
        thread_count: usize,
        /// æ˜¯å¦åœ¨tokioè¿è¡Œæ—¶å†…
        /// Whether inside tokio runtime
        in_tokio_runtime: AtomicBool,
        /// æ€§èƒ½é˜ˆå€¼ï¼šå°äºæ­¤æ‰¹é‡å¤§å°æ—¶ä½¿ç”¨ç›´é€šæ¨¡å¼
        /// Performance threshold: use bypass mode for batches smaller than this
        bypass_threshold: usize,
    }
    
    impl Default for ExecutionModeSelector {
        fn default() -> Self {
            Self::new()
        }
    }

    impl ExecutionModeSelector {
        pub fn new() -> Self {
            Self {
                thread_count: num_cpus::get(),
                in_tokio_runtime: AtomicBool::new(false),
                bypass_threshold: 128, // å°æ‰¹é‡ä½¿ç”¨ç›´é€šæ¨¡å¼
            }
        }
        
        /// æ ¹æ®æ‰¹é‡å¤§å°å’Œç³»ç»ŸçŠ¶æ€é€‰æ‹©æœ€ä¼˜æ‰§è¡Œæ¨¡å¼
        /// Choose optimal execution mode based on batch size and system state
        pub fn choose_mode(&self, batch_size: usize) -> ExecutionMode {
            // æ£€æµ‹æ˜¯å¦åœ¨tokioè¿è¡Œæ—¶ä¸­
            let in_runtime = tokio::runtime::Handle::try_current().is_ok();
            self.in_tokio_runtime.store(in_runtime, Ordering::Relaxed);
            
            match (batch_size, self.thread_count, in_runtime) {
                // å°æ‰¹é‡ + å•æ ¸å¿ƒ + éå¼‚æ­¥ç¯å¢ƒ = ç›´é€š
                (size, 1, false) if size <= self.bypass_threshold => ExecutionMode::SingleThreadDirect,
                
                // å°æ‰¹é‡ + æ— éœ€å¹¶è¡Œ = ç›´é€š  
                (size, _, _) if size <= 64 => ExecutionMode::SingleThreadDirect,
                
                // ä¸­ç­‰æ‰¹é‡ + å¤šæ ¸å¿ƒ = å¹¶è¡Œ
                (size, cores, _) if size > self.bypass_threshold && cores > 1 => ExecutionMode::MultiThreadParallel,
                
                // å…¶ä»–æƒ…å†µï¼šå•çº¿ç¨‹å¼‚æ­¥
                _ => ExecutionMode::SingleThreadAsync,
            }
        }
        
        /// æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ç›´é€šæ¨¡å¼
        /// Check if bypass mode should be used
        pub fn should_bypass_async(&self, batch_size: usize) -> bool {
            matches!(self.choose_mode(batch_size), ExecutionMode::SingleThreadDirect)
        }
    }
    
    /// ç›´é€šå¼å®šæ—¶å™¨å¤„ç†å™¨ï¼ˆé›¶å¼‚æ­¥å¼€é”€ï¼‰
    /// Bypass timer processor (zero async overhead)
    pub struct BypassTimerProcessor {
        /// å†…è”å¤„ç†ç¼“å†²åŒºï¼ˆæ ˆåˆ†é…ï¼‰
        /// Inline processing buffer (stack allocated)
        inline_buffer: Vec<ProcessedTimerData>,
        /// å¤„ç†ç»Ÿè®¡
        /// Processing statistics
        processed_count: usize,
    }

    impl Default for BypassTimerProcessor {
        fn default() -> Self {
            Self::new()
        }
    }
    
    impl BypassTimerProcessor {
        pub fn new() -> Self {
            Self {
                inline_buffer: Vec::with_capacity(256), // é¢„åˆ†é…é¿å…è¿è¡Œæ—¶åˆ†é…
                processed_count: 0,
            }
        }
        
        /// ç›´é€šå¼æ‰¹é‡å¤„ç†ï¼ˆåŒæ­¥ï¼Œé›¶å¼‚æ­¥å¼€é”€ï¼‰
        /// Bypass batch processing (synchronous, zero async overhead)
        pub fn process_batch_bypass(
            &mut self,
            timer_entries: &[TimerEntry],
        ) -> &[ProcessedTimerData] {
            self.inline_buffer.clear();
            self.inline_buffer.reserve(timer_entries.len());
            
            // ç›´æ¥åŒæ­¥å¤„ç†ï¼Œæ— å¼‚æ­¥è°ƒåº¦å¼€é”€
            // Direct synchronous processing, no async scheduling overhead
            for (i, entry) in timer_entries.iter().enumerate() {
                self.inline_buffer.push(ProcessedTimerData {
                    entry_id: entry.id,
                    connection_id: entry.event.data.connection_id,
                    timeout_event: entry.event.data.timeout_event,
                    expiry_time: entry.expiry_time,
                    slot_index: i,
                });
            }
            
            self.processed_count += timer_entries.len();
            &self.inline_buffer
        }
        
        /// ç›´é€šå¼äº‹ä»¶åˆ†å‘ï¼ˆåŒæ­¥ï¼Œä½¿ç”¨å¼•ç”¨ä¼ é€’ï¼‰
        /// Bypass event dispatch (synchronous, using reference passing)
        pub fn dispatch_events_bypass<H>(
            &self,
            processed_data: &[ProcessedTimerData],
            handler: &H,
        ) -> usize
        where
            H: ZeroCopyEventDelivery,
        {
            // æ„å»ºäº‹ä»¶å¼•ç”¨æ•°ç»„ï¼Œé¿å…å…‹éš†
            // Build event reference array, avoiding clones
            let event_refs: Vec<TimerEventData> = processed_data.iter()
                .map(|data| TimerEventData::new(data.connection_id, data.timeout_event))
                .collect();
            
            let event_ref_ptrs: Vec<&TimerEventData> = event_refs.iter().collect();
            
            // æ‰¹é‡ä¼ é€’å¼•ç”¨ï¼Œé›¶æ‹·è´
            // Batch deliver references, zero-copy
            handler.batch_deliver_event_refs(&event_ref_ptrs)
        }
        
        /// è·å–å¤„ç†ç»Ÿè®¡
        /// Get processing statistics
        pub fn get_processed_count(&self) -> usize {
            self.processed_count
        }
    }
}

/// å†…å­˜é¢„åˆ†é…ç®¡ç†å™¨ - å‡å°‘è¿è¡Œæ—¶åˆ†é…
/// Memory pre-allocation manager - reducing runtime allocations
pub mod memory_optimization {
    use super::*;
    
    /// é¢„åˆ†é…å†…å­˜æ± 
    /// Pre-allocated memory pool  
    pub struct MemoryPool {
        /// é¢„åˆ†é…çš„æ•°æ®ç¼“å†²åŒº
        /// Pre-allocated data buffers
        data_buffers: Vec<Vec<ProcessedTimerData>>,
        /// é¢„åˆ†é…çš„IDç¼“å†²åŒº  
        /// Pre-allocated ID buffers
        id_buffers: Vec<Vec<u32>>,
        /// é¢„åˆ†é…çš„æ—¶é—´æˆ³ç¼“å†²åŒº
        /// Pre-allocated timestamp buffers
        timestamp_buffers: Vec<Vec<u64>>,
        /// å½“å‰ç¼“å†²åŒºç´¢å¼•
        /// Current buffer index
        current_buffer_index: AtomicUsize,
        /// ç¼“å†²åŒºæ•°é‡
        /// Buffer count
        buffer_count: usize,
    }
    
    impl MemoryPool {
        pub fn new(buffer_count: usize, buffer_capacity: usize) -> Self {
            let mut data_buffers = Vec::with_capacity(buffer_count);
            let mut id_buffers = Vec::with_capacity(buffer_count);
            let mut timestamp_buffers = Vec::with_capacity(buffer_count);
            
            for _ in 0..buffer_count {
                data_buffers.push(Vec::with_capacity(buffer_capacity));
                id_buffers.push(Vec::with_capacity(buffer_capacity));
                timestamp_buffers.push(Vec::with_capacity(buffer_capacity));
            }
            
            Self {
                data_buffers,
                id_buffers,
                timestamp_buffers,
                current_buffer_index: AtomicUsize::new(0),
                buffer_count,
            }
        }
        
        /// è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ•°æ®ç¼“å†²åŒº
        /// Get next available data buffer
        pub fn get_data_buffer(&mut self) -> &mut Vec<ProcessedTimerData> {
            let index = self.current_buffer_index.fetch_add(1, Ordering::Relaxed) % self.buffer_count;
            let buffer = &mut self.data_buffers[index];
            buffer.clear();
            buffer
        }
        
        /// è·å–IDç¼“å†²åŒºå’Œæ—¶é—´æˆ³ç¼“å†²åŒº
        /// Get ID buffer and timestamp buffer
        pub fn get_work_buffers(&mut self) -> (&mut Vec<u32>, &mut Vec<u64>) {
            let index = self.current_buffer_index.load(Ordering::Relaxed) % self.buffer_count;
            let id_buffer = &mut self.id_buffers[index];
            let timestamp_buffer = &mut self.timestamp_buffers[index];
            
            id_buffer.clear();
            timestamp_buffer.clear();
            
            (id_buffer, timestamp_buffer)
        }
        
        /// è¿”å›ç¼“å†²åŒºï¼ˆæ ‡è®°ä¸ºå¯é‡ç”¨ï¼‰
        /// Return buffer (mark as reusable)
        pub fn return_buffers(&self) {
            // åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œç¼“å†²åŒºä¼šåœ¨ä¸‹æ¬¡è·å–æ—¶è‡ªåŠ¨æ¸…ç†
            // In this implementation, buffers are automatically cleaned on next acquisition
        }
    }
    
    /// é›¶åˆ†é…å¤„ç†å™¨ - æœ€å°åŒ–å†…å­˜åˆ†é…
    /// Zero-allocation processor - minimizing memory allocations
    pub struct ZeroAllocProcessor {
        /// å†…å­˜æ± 
        /// Memory pool
        memory_pool: MemoryPool,
        /// æ ˆåˆ†é…ç¼“å†²åŒºï¼ˆç”¨äºå°æ‰¹é‡ï¼‰
        /// Stack-allocated buffer (for small batches)
        stack_buffer: [ProcessedTimerData; 64],
        /// æ ˆç¼“å†²åŒºä½¿ç”¨è®¡æ•°
        /// Stack buffer usage count
        stack_usage: usize,
    }

    impl Default for ZeroAllocProcessor {
        fn default() -> Self {
            Self::new()
        }
    }
    
    impl ZeroAllocProcessor {
        pub fn new() -> Self {
            // åˆ›å»ºé»˜è®¤æ—¶é—´æˆ³
            let default_instant = tokio::time::Instant::now();
            
            // ä½¿ç”¨æ•°ç»„æ˜ å°„æ¥åˆå§‹åŒ–æ ˆç¼“å†²åŒº
            let stack_buffer: [ProcessedTimerData; 64] = std::array::from_fn(|_| ProcessedTimerData {
                entry_id: 0,
                connection_id: 0,
                timeout_event: TimeoutEvent::IdleTimeout,
                expiry_time: default_instant,
                slot_index: 0,
            });
            
            Self {
                memory_pool: MemoryPool::new(4, 1024), // 4ä¸ªç¼“å†²åŒºï¼Œæ¯ä¸ª1024å®¹é‡
                stack_buffer,
                stack_usage: 0,
            }
        }
        
        /// å¤„ç†å°æ‰¹é‡ï¼ˆä½¿ç”¨æ ˆåˆ†é…ï¼‰
        /// Process small batch (using stack allocation)
        pub fn process_small_batch(&mut self, timer_entries: &[TimerEntry]) -> &[ProcessedTimerData] {
            if timer_entries.len() <= 64 {
                self.stack_usage = timer_entries.len();
                
                for (i, entry) in timer_entries.iter().enumerate() {
                    self.stack_buffer[i] = ProcessedTimerData {
                        entry_id: entry.id,
                        connection_id: entry.event.data.connection_id,
                        timeout_event: entry.event.data.timeout_event,
                        expiry_time: entry.expiry_time,
                        slot_index: i,
                    };
                }
                
                &self.stack_buffer[..self.stack_usage]
            } else {
                &[]
            }
        }
        
        /// å¤„ç†å¤§æ‰¹é‡ï¼ˆä½¿ç”¨å†…å­˜æ± ï¼‰
        /// Process large batch (using memory pool)
        pub fn process_large_batch(&mut self, timer_entries: &[TimerEntry]) -> &[ProcessedTimerData] {
            let buffer = self.memory_pool.get_data_buffer();
            buffer.reserve(timer_entries.len());
            
            for (i, entry) in timer_entries.iter().enumerate() {
                buffer.push(ProcessedTimerData {
                    entry_id: entry.id,
                    connection_id: entry.event.data.connection_id,
                    timeout_event: entry.event.data.timeout_event,
                    expiry_time: entry.expiry_time,
                    slot_index: i,
                });
            }
            
            buffer
        }
    }
}

/// è‡ªé€‚åº”å¹¶è¡Œç­–ç•¥é€‰æ‹©
/// Adaptive parallel strategy selection
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum OptimalParallelStrategy {
    /// ä»…SIMDä¼˜åŒ– - å°æ‰¹é‡ (<256)
    /// SIMD Only - Small batches (<256)
    SIMDOnly,
    /// SIMD + Rayon - ä¸­æ‰¹é‡ (256-4096)
    /// SIMD + Rayon - Medium batches (256-4096)
    SIMDWithRayon,
    /// å®Œæ•´æ··åˆç­–ç•¥ - å¤§æ‰¹é‡ (>4096)
    /// Full Hybrid - Large batches (>4096)
    FullHybrid,
}

/// æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿçš„æ ¸å¿ƒ
/// Core of the hybrid parallel timer system
/// 
/// è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚ä¼˜åŒ–ç­–ç•¥ï¼š
/// 1. ä¼˜å…ˆä½¿ç”¨é›¶æ‹·è´åˆ†å‘å™¨è·å¾—æœ€ä½³æ€§èƒ½
/// 2. å½“é›¶æ‹·è´åˆ†å‘å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨fallbackåˆ°å¼‚æ­¥åˆ†å‘å™¨ç¡®ä¿å¯é æ€§
/// 3. é€šè¿‡æ™ºèƒ½ç­–ç•¥é€‰æ‹©å™¨è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜æ‰§è¡Œè·¯å¾„
pub struct HybridParallelTimerSystem {
    /// SIMDå¤„ç†å™¨
    simd_processor: SIMDTimerProcessor,
    /// Rayonæ‰¹é‡æ‰§è¡Œå™¨
    rayon_executor: RayonBatchExecutor,
    /// å¼‚æ­¥äº‹ä»¶åˆ†å‘å™¨ (ç”¨ä½œé›¶æ‹·è´åˆ†å‘çš„fallbackæœºåˆ¶)
    /// Async event dispatcher (used as fallback for zero-copy dispatch)
    async_dispatcher: Arc<AsyncEventDispatcher>,
    /// é›¶æ‹·è´äº‹ä»¶åˆ†å‘å™¨ (ä¸»è¦çš„äº‹ä»¶åˆ†å‘ç­–ç•¥)
    /// Zero-copy event dispatcher (primary event dispatch strategy)
    zero_copy_dispatcher: crate::timer::event::zero_copy::ZeroCopyBatchDispatcher,
    /// å•çº¿ç¨‹ç›´é€šå¤„ç†å™¨
    bypass_processor: single_thread_bypass::BypassTimerProcessor,
    /// æ‰§è¡Œæ¨¡å¼é€‰æ‹©å™¨
    mode_selector: single_thread_bypass::ExecutionModeSelector,
    /// é›¶åˆ†é…å¤„ç†å™¨
    zero_alloc_processor: memory_optimization::ZeroAllocProcessor,
    /// æ€§èƒ½ç»Ÿè®¡
    stats: ParallelProcessingStats,
    /// CPUæ ¸å¿ƒæ•°ï¼Œç”¨äºç­–ç•¥é€‰æ‹©
    cpu_cores: usize,
}

/// SIMDå®šæ—¶å™¨å¤„ç†å™¨
/// SIMD Timer Processor
pub struct SIMDTimerProcessor {
    /// æ‰¹é‡è®¡ç®—ç¼“å†²åŒº
    #[allow(dead_code)] // é¢„ç•™ç”¨äºæœªæ¥çš„è®¡ç®—ä¼˜åŒ–
    computation_buffer: Vec<u64>,
    /// ç»“æœç¼“å­˜
    result_cache: Vec<ProcessedTimerData>,
}

/// Rayonæ‰¹é‡æ‰§è¡Œå™¨
/// Rayon Batch Executor  
pub struct RayonBatchExecutor {
    /// å·¥ä½œçªƒå–é˜Ÿåˆ—çš„å¤§å°
    chunk_size: usize,
    /// çº¿ç¨‹æ± å¤§å°
    thread_pool_size: usize,
}

/// å¼‚æ­¥äº‹ä»¶åˆ†å‘å™¨
/// Async Event Dispatcher
pub struct AsyncEventDispatcher {
    /// äº‹ä»¶å‘é€é€šé“æ± 
    event_channels: Vec<mpsc::Sender<TimerEventData>>,
    /// è´Ÿè½½å‡è¡¡ç´¢å¼•
    round_robin_index: Mutex<usize>,
}

/// å¤„ç†åçš„å®šæ—¶å™¨æ•°æ®
/// Processed timer data
#[derive(Debug, Clone)]
pub struct ProcessedTimerData {
    pub entry_id: TimerEntryId,
    pub connection_id: ConnectionId,
    pub timeout_event: TimeoutEvent,
    pub expiry_time: tokio::time::Instant,
    pub slot_index: usize,
}

/// æ‰¹é‡å¹¶è¡Œå¤„ç†ç»“æœ
/// Batch parallel processing result
#[derive(Debug)]
pub struct ParallelProcessingResult {
    /// æˆåŠŸå¤„ç†çš„å®šæ—¶å™¨æ•°é‡
    pub processed_count: usize,
    /// å¤„ç†è€—æ—¶
    pub processing_duration: Duration,
    /// ä½¿ç”¨çš„ç­–ç•¥
    pub strategy_used: OptimalParallelStrategy,
    /// è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    pub detailed_stats: DetailedProcessingStats,
}

/// è¯¦ç»†å¤„ç†ç»Ÿè®¡ä¿¡æ¯
/// Detailed processing statistics
#[derive(Debug, Default)]
pub struct DetailedProcessingStats {
    pub simd_operations: usize,
    pub rayon_chunks_processed: usize,
    pub async_dispatches: usize,
    pub cache_hits: usize,
    pub memory_allocations: usize,
}

/// å¹¶è¡Œå¤„ç†æ€§èƒ½ç»Ÿè®¡
/// Parallel processing performance statistics
#[derive(Debug, Default)]
pub struct ParallelProcessingStats {
    pub total_batches_processed: u64,
    pub simd_only_count: u64,
    pub simd_rayon_count: u64,
    pub full_hybrid_count: u64,
    pub avg_processing_time_ns: f64,
    pub peak_throughput_ops_per_sec: f64,
}

impl HybridParallelTimerSystem {
    /// åˆ›å»ºæ–°çš„æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿ
    /// Create a new hybrid parallel timer system
    pub fn new() -> Self {
        let cpu_cores = num_cpus::get();
        
        Self {
            simd_processor: SIMDTimerProcessor::new(),
            rayon_executor: RayonBatchExecutor::new(cpu_cores),
            async_dispatcher: Arc::new(AsyncEventDispatcher::new()),
            zero_copy_dispatcher: crate::timer::event::zero_copy::ZeroCopyBatchDispatcher::new(1024, cpu_cores),
            bypass_processor: single_thread_bypass::BypassTimerProcessor::new(),
            mode_selector: single_thread_bypass::ExecutionModeSelector::new(),
            zero_alloc_processor: memory_optimization::ZeroAllocProcessor::new(),
            stats: ParallelProcessingStats::default(),
            cpu_cores,
        }
    }

    /// é€‰æ‹©æœ€ä¼˜çš„å¹¶è¡Œç­–ç•¥ï¼ˆå¼‚æ­¥å¼€é”€ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// Choose the optimal parallel strategy (async overhead optimized version)
    pub fn choose_optimal_strategy(&self, batch_size: usize) -> OptimalParallelStrategy {
        match batch_size {
            0..=256 => OptimalParallelStrategy::SIMDOnly,
            257..=1023 => {
                // ä¸­ç­‰æ‰¹é‡ï¼šä¼˜å…ˆä½¿ç”¨SIMDï¼Œé¿å…Rayonçš„å¼‚æ­¥å¼€é”€
                OptimalParallelStrategy::SIMDOnly
            }
            1024..=4095 => {
                // ä¸­å¤§æ‰¹é‡ï¼šä½¿ç”¨FullHybridç­–ç•¥é¿å…Rayonå¼‚æ­¥åŒ…è£…å¼€é”€
                if self.cpu_cores >= 2 {
                    OptimalParallelStrategy::FullHybrid
                } else {
                    OptimalParallelStrategy::SIMDOnly
                }
            }
            4096.. => {
                // å¤§æ‰¹é‡ï¼šä½¿ç”¨å®Œæ•´æ··åˆç­–ç•¥è·å¾—æœ€ä½³æ€§èƒ½
                if self.cpu_cores >= 2 {
                    OptimalParallelStrategy::FullHybrid
                } else {
                    OptimalParallelStrategy::SIMDOnly
                }
            }
        }
    }

    /// å¹¶è¡Œå¤„ç†å®šæ—¶å™¨æ‰¹é‡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// Process timer batch in parallel (optimized version)
    pub async fn process_timer_batch(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ParallelProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let batch_size = timer_entries.len();
        let start_time = Instant::now();

        // é¦–å…ˆæ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å•çº¿ç¨‹ç›´é€šæ¨¡å¼
        // First check if single-thread bypass mode should be used
        if self.mode_selector.should_bypass_async(batch_size) {
            return self.process_bypass_mode(timer_entries, start_time).await;
        }

        // å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„å¹¶è¡Œç­–ç•¥
        // Otherwise use traditional parallel strategy
        let strategy = self.choose_optimal_strategy(batch_size);
        let result = match strategy {
            OptimalParallelStrategy::SIMDOnly => {
                self.process_simd_only_optimized(timer_entries).await?
            }
            OptimalParallelStrategy::SIMDWithRayon => {
                self.process_simd_with_rayon_optimized(timer_entries).await?
            }
            OptimalParallelStrategy::FullHybrid => {
                self.process_full_hybrid_optimized(timer_entries).await?
            }
        };

        let processing_duration = start_time.elapsed();
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.update_stats(strategy, batch_size, processing_duration);

        Ok(ParallelProcessingResult {
            processed_count: result.processed_count,
            processing_duration,
            strategy_used: strategy,
            detailed_stats: result.detailed_stats,
        })
    }

    /// å•çº¿ç¨‹ç›´é€šæ¨¡å¼å¤„ç†ï¼ˆé›¶å¼‚æ­¥å¼€é”€ï¼‰
    /// Single-thread bypass mode processing (zero async overhead)
    async fn process_bypass_mode(
        &mut self,
        timer_entries: Vec<TimerEntry>,
        start_time: Instant,
    ) -> Result<ParallelProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let batch_size = timer_entries.len();
        
        // æ ¹æ®æ‰¹é‡å¤§å°é€‰æ‹©å†…å­˜ä¼˜åŒ–ç­–ç•¥
        // Choose memory optimization strategy based on batch size
        let processed_data = if batch_size <= 64 {
            // å°æ‰¹é‡ï¼šä½¿ç”¨æ ˆåˆ†é…
            // Small batch: use stack allocation
            self.zero_alloc_processor.process_small_batch(&timer_entries)
        } else {
            // å¤§æ‰¹é‡ï¼šä½¿ç”¨å†…å­˜æ± 
            // Large batch: use memory pool
            self.zero_alloc_processor.process_large_batch(&timer_entries)
        };

        // ç›´é€šå¼äº‹ä»¶åˆ†å‘ï¼ˆåŒæ­¥ï¼Œä½¿ç”¨é›¶æ‹·è´ï¼‰
        // Bypass event dispatch (synchronous, using zero-copy)
        let dummy_handler = crate::timer::event::zero_copy::RefEventHandler::new(|_event_ref: &TimerEventData| true);
        let dispatch_count = self.bypass_processor.dispatch_events_bypass(processed_data, &dummy_handler);

        let processing_duration = start_time.elapsed();
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆç›´é€šæ¨¡å¼æ ‡è®°ä¸ºSIMDOnlyç­–ç•¥ï¼‰
        self.update_stats(OptimalParallelStrategy::SIMDOnly, batch_size, processing_duration);

        Ok(ParallelProcessingResult {
            processed_count: batch_size,
            processing_duration,
            strategy_used: OptimalParallelStrategy::SIMDOnly, // æ ‡è®°ä¸ºSIMDç­–ç•¥ä½†å®é™…æ˜¯ç›´é€š
            detailed_stats: DetailedProcessingStats {
                simd_operations: 0, // ç›´é€šæ¨¡å¼ä¸ä½¿ç”¨SIMD
                async_dispatches: dispatch_count,
                memory_allocations: if batch_size <= 64 { 0 } else { 1 }, // æ ˆåˆ†é…æˆ–å†…å­˜æ± 
                ..Default::default()
            },
        })
    }

    /// ä»…ä½¿ç”¨SIMDå¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// Process using SIMD only (optimized version)
    async fn process_simd_only_optimized(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let processed_data = self.simd_processor.process_batch(&timer_entries)?;
        
        // ä¼˜å…ˆä½¿ç”¨é›¶æ‹·è´åˆ†å‘å™¨ï¼Œå¤±è´¥æ—¶fallbackåˆ°å¼‚æ­¥åˆ†å‘å™¨
        // Prefer zero-copy dispatcher, fallback to async dispatcher on failure
        let events: Vec<TimerEventData> = processed_data.iter()
            .map(|data| TimerEventData::new(data.connection_id, data.timeout_event))
            .collect();
        
        let dispatch_count = self.zero_copy_dispatcher.batch_dispatch_events(events.clone());
        
        // å¦‚æœé›¶æ‹·è´åˆ†å‘å¤±è´¥ï¼ˆè¿”å›0ï¼‰ï¼Œä½¿ç”¨å¼‚æ­¥åˆ†å‘å™¨ä½œä¸ºfallback
        // If zero-copy dispatch fails (returns 0), use async dispatcher as fallback
        let final_dispatch_count = if dispatch_count == 0 {
            tracing::warn!("Zero-copy dispatch failed, falling back to async dispatcher");
            self.async_dispatcher.dispatch_timer_events(processed_data.clone()).await.unwrap_or(0)
        } else {
            dispatch_count
        };

        Ok(ProcessingResult {
            processed_count: timer_entries.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: timer_entries.len() / 8 + 1, // u32x8
                async_dispatches: final_dispatch_count,
                memory_allocations: 1, // ä¸€æ¬¡æ‰¹é‡åˆ†é…
                ..Default::default()
            },
        })
    }

    /// ä½¿ç”¨SIMD + Rayonå¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// Process using SIMD + Rayon (optimized version)
    async fn process_simd_with_rayon_optimized(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        // ä½¿ç”¨Rayonå¹¶è¡Œå¤„ç†æ•°æ®
        let processed_data = self.rayon_executor
            .parallel_process_with_simd(timer_entries, &mut self.simd_processor)
            .await?;

        // ä¼˜å…ˆä½¿ç”¨é›¶æ‹·è´æ‰¹é‡åˆ†å‘ï¼Œå¤±è´¥æ—¶fallbackåˆ°å¼‚æ­¥åˆ†å‘å™¨
        // Prefer zero-copy batch dispatch, fallback to async dispatcher on failure
        let events: Vec<TimerEventData> = processed_data.iter()
            .map(|data| TimerEventData::new(data.connection_id, data.timeout_event))
            .collect();
        
        let dispatch_count = self.zero_copy_dispatcher.batch_dispatch_events(events.clone());
        
        // å¦‚æœé›¶æ‹·è´åˆ†å‘å¤±è´¥ï¼Œä½¿ç”¨å¼‚æ­¥åˆ†å‘å™¨ä½œä¸ºfallback
        // If zero-copy dispatch fails, use async dispatcher as fallback
        let final_dispatch_count = if dispatch_count == 0 {
            tracing::warn!("Zero-copy dispatch failed, falling back to async dispatcher");
            self.async_dispatcher.dispatch_timer_events(processed_data.clone()).await.unwrap_or(0)
        } else {
            dispatch_count
        };

        Ok(ProcessingResult {
            processed_count: processed_data.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: processed_data.len() / 8 + 1,
                rayon_chunks_processed: processed_data.len().div_ceil(512), // 512 per chunk
                async_dispatches: final_dispatch_count,
                memory_allocations: 1, // ä¸€æ¬¡æ‰¹é‡åˆ†é…
                ..Default::default()
            },
        })
    }

    /// ä½¿ç”¨å®Œæ•´æ··åˆç­–ç•¥å¤„ç†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    /// Process using full hybrid strategy (optimized version)
    async fn process_full_hybrid_optimized(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let batch_size = timer_entries.len();
        
        // å¯¹äºä¸­ç­‰æ‰¹é‡(1024-4095)ï¼Œä½¿ç”¨ç›´æ¥åŒæ­¥è·¯å¾„é¿å…spawn_blockingå¼€é”€
        // For medium batches (1024-4095), use direct sync path to avoid spawn_blocking overhead
        let processed_data = if batch_size <= 4095 && self.cpu_cores >= 2 {
            // ç›´æ¥åœ¨å½“å‰ä»»åŠ¡ä¸­ä½¿ç”¨Rayonï¼Œé¿å…spawn_blockingçš„ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
            // Direct Rayon usage in current task, avoiding spawn_blocking context switch overhead
            self.rayon_executor.parallel_process_with_simd_sync(timer_entries, &mut self.simd_processor)?
        } else {
            // å¤§æ‰¹é‡ä½¿ç”¨spawn_blockingé¿å…é˜»å¡å¼‚æ­¥è¿è¡Œæ—¶
            // Large batches use spawn_blocking to avoid blocking async runtime
            tokio::task::spawn_blocking({
                let mut rayon_executor = self.rayon_executor.clone();
                let mut simd_processor = self.simd_processor.clone();
                move || -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
                    let result = rayon_executor.parallel_process_with_simd_sync(timer_entries, &mut simd_processor)?;
                    Ok(result)
                }
            }).await??
        };

        // æ­¥éª¤2: ä¼˜å…ˆä½¿ç”¨é›¶æ‹·è´æ‰¹é‡åˆ†å‘ï¼Œå¤±è´¥æ—¶fallbackåˆ°å¼‚æ­¥åˆ†å‘å™¨
        // Step 2: Prefer zero-copy batch dispatch, fallback to async dispatcher on failure
        let events: Vec<TimerEventData> = processed_data.iter()
            .map(|data| TimerEventData::new(data.connection_id, data.timeout_event))
            .collect();
        
        let dispatch_count = self.zero_copy_dispatcher.batch_dispatch_events(events.clone());
        
        // å¦‚æœé›¶æ‹·è´åˆ†å‘å¤±è´¥ï¼Œä½¿ç”¨å¼‚æ­¥åˆ†å‘å™¨ä½œä¸ºfallback
        // If zero-copy dispatch fails, use async dispatcher as fallback
        let total_dispatches = if dispatch_count == 0 {
            tracing::warn!("Zero-copy dispatch failed in full hybrid mode, falling back to async dispatcher");
            self.async_dispatcher.dispatch_timer_events(processed_data.clone()).await.unwrap_or(0)
        } else {
            dispatch_count
        };

        Ok(ProcessingResult {
            processed_count: processed_data.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: processed_data.len() / 8 + 1,
                rayon_chunks_processed: processed_data.len().div_ceil(512),
                async_dispatches: total_dispatches,
                memory_allocations: if batch_size <= 4095 { 1 } else { 2 }, // ç›´æ¥è·¯å¾„vs spawn_blocking
                ..Default::default()
            },
        })
    }


    /// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    /// å°†ç»Ÿè®¡é€»è¾‘é‡æ„åˆ°æ­¤å‡½æ•°ä¸­ï¼Œç»“æ„æ›´æ¸…æ™°
    fn update_stats(&mut self, strategy: OptimalParallelStrategy, batch_size: usize, duration: Duration) {
        self.stats.total_batches_processed += 1;
        self.stats.avg_processing_time_ns = (self.stats.avg_processing_time_ns + duration.as_nanos() as f64) / 2.0;

        match strategy {
            OptimalParallelStrategy::SIMDOnly => self.stats.simd_only_count += 1,
            OptimalParallelStrategy::SIMDWithRayon => self.stats.simd_rayon_count += 1,
            OptimalParallelStrategy::FullHybrid => self.stats.full_hybrid_count += 1,
        }

        // ä½¿ç”¨f64è¿›è¡Œç²¾ç¡®çš„ååé‡è®¡ç®—
        let duration_secs = duration.as_secs_f64();
        if duration_secs > 0.0 {
            let current_throughput = batch_size as f64 / duration_secs;
            if current_throughput > self.stats.peak_throughput_ops_per_sec {
                self.stats.peak_throughput_ops_per_sec = current_throughput;
            }
        }
    }

    /// è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    /// Get performance statistics
    pub fn get_stats(&self) -> &ParallelProcessingStats {
        &self.stats
    }

    /// ç›´æ¥ä½¿ç”¨å¼‚æ­¥åˆ†å‘å™¨å¤„ç†äº‹ä»¶ï¼ˆç”¨äºç‰¹æ®Šåœºæ™¯æˆ–æµ‹è¯•ï¼‰
    /// Directly use async dispatcher for events (for special scenarios or testing)
    pub async fn dispatch_events_async(
        &self,
        processed_data: Vec<ProcessedTimerData>,
    ) -> Result<usize, Box<dyn std::error::Error + Send + Sync>> {
        self.async_dispatcher.dispatch_timer_events(processed_data).await
    }

    /// è·å–å¼‚æ­¥åˆ†å‘å™¨çš„å¼•ç”¨ï¼ˆç”¨äºé«˜çº§ç”¨é€”ï¼‰
    /// Get async dispatcher reference (for advanced usage)
    pub fn get_async_dispatcher(&self) -> &Arc<AsyncEventDispatcher> {
        &self.async_dispatcher
    }
}

/// å†…éƒ¨å¤„ç†ç»“æœç±»å‹
/// Internal processing result type
struct ProcessingResult {
    processed_count: usize,
    detailed_stats: DetailedProcessingStats,
}

impl Default for SIMDTimerProcessor {
    fn default() -> Self {
        Self::new()
    }
}

impl SIMDTimerProcessor {
    pub fn new() -> Self {
        Self {
            computation_buffer: Vec::with_capacity(8192),
            result_cache: Vec::with_capacity(4096),
        }
    }

    /// ä½¿ç”¨SIMDæ‰¹é‡å¤„ç†å®šæ—¶å™¨
    /// Process timers in batch using SIMD
    pub fn process_batch(
        &mut self,
        timer_entries: &[TimerEntry],
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        self.result_cache.clear();
        self.result_cache.reserve(timer_entries.len());

        // é¢„å¤„ç†ï¼šæå–è¿æ¥IDç”¨äºu32x8å‘é‡åŒ–
        let connection_ids: Vec<u32> = timer_entries
            .iter()
            .map(|entry| entry.event.data.connection_id) 
            .collect();

        // ä½¿ç”¨u32x8å¹¶è¡Œå¤„ç†è¿æ¥ID
        self.simd_process_connection_ids(&connection_ids)?;

        // æå–æ—¶é—´æˆ³ç”¨äºu64x4å‘é‡åŒ–  
        let expiry_times: Vec<u64> = timer_entries
            .iter()
            .map(|entry| {
                // å°†tokio::time::Instantè½¬æ¢ä¸ºçº³ç§’æ—¶é—´æˆ³
                entry.expiry_time.elapsed().as_nanos() as u64
            })
            .collect();

        // ä½¿ç”¨u64x4å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
        self.simd_process_timestamps(&expiry_times)?;

        // ç»„è£…æœ€ç»ˆç»“æœ
        for (i, entry) in timer_entries.iter().enumerate() {
            self.result_cache.push(ProcessedTimerData {
                entry_id: entry.id,
                connection_id: entry.event.data.connection_id,
                timeout_event: entry.event.data.timeout_event,
                expiry_time: entry.expiry_time,
                slot_index: i, // ç®€åŒ–çš„æ§½ä½ç´¢å¼•
            });
        }

        Ok(self.result_cache.clone())
    }

    /// ä½¿ç”¨u32x8å¹¶è¡Œå¤„ç†è¿æ¥ID
    /// Process connection IDs in parallel using u32x8
    pub fn simd_process_connection_ids(
        &mut self,
        connection_ids: &[u32],
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let mut i = 0;
        
        // 8è·¯å¹¶è¡Œå¤„ç†è¿æ¥ID
        while i + 8 <= connection_ids.len() {
            let id_vec = u32x8::new([
                connection_ids[i], connection_ids[i + 1], connection_ids[i + 2], connection_ids[i + 3],
                connection_ids[i + 4], connection_ids[i + 5], connection_ids[i + 6], connection_ids[i + 7],
            ]);
            
            // SIMDå¹¶è¡ŒéªŒè¯å’Œå¤„ç†
            let processed_ids = id_vec.to_array();
            
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šSIMDå¤„ç†é€»è¾‘
            for &processed_id in &processed_ids {
                // éªŒè¯è¿æ¥IDæœ‰æ•ˆæ€§ç­‰
                if processed_id > 0 && processed_id < 1_000_000 {
                    // æœ‰æ•ˆè¿æ¥IDçš„å¤„ç†é€»è¾‘
                }
            }
            
            i += 8;
        }
        
        // å¤„ç†å‰©ä½™çš„è¿æ¥ID
        while i < connection_ids.len() {
            // æ ‡é‡å¤„ç†å‰©ä½™å…ƒç´ 
            i += 1;
        }
        
        Ok(())
    }

    /// ä½¿ç”¨u64x4å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
    /// Process timestamps in parallel using u64x4
    pub fn simd_process_timestamps(
        &mut self,
        timestamps: &[u64],
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let mut i = 0;
        
        // 4è·¯å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
        while i + 4 <= timestamps.len() {
            let time_vec = u64x4::new([
                timestamps[i], timestamps[i + 1], timestamps[i + 2], timestamps[i + 3]
            ]);
            
            // SIMDå¹¶è¡Œæ—¶é—´è®¡ç®—
            let current_time_vec = u64x4::splat(
                tokio::time::Instant::now().elapsed().as_nanos() as u64
            );
            
            // è®¡ç®—æ—¶é—´å·®
            let time_diffs = time_vec - current_time_vec;
            let _diff_array = time_diffs.to_array();
            
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ—¶é—´ç›¸å…³çš„SIMDè®¡ç®—
            
            i += 4;
        }
        
        // å¤„ç†å‰©ä½™æ—¶é—´æˆ³
        while i < timestamps.len() {
            // æ ‡é‡å¤„ç†å‰©ä½™å…ƒç´ 
            i += 1;
        }
        
        Ok(())
    }
}
/// å…‹éš†å¤„ç†å™¨ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
/// Clone processor (for multi-thread)
impl Clone for SIMDTimerProcessor {
    fn clone(&self) -> Self {
        Self::new()
    }
}

impl RayonBatchExecutor {
    pub fn new(cpu_cores: usize) -> Self {
        Self {
            chunk_size: 512.max(cpu_cores * 64), // æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´å—å¤§å°
            thread_pool_size: cpu_cores,
        }
    }

    /// ä½¿ç”¨Rayon + SIMDå¹¶è¡Œå¤„ç† (å¼‚æ­¥ç‰ˆæœ¬)
    /// Parallel processing using Rayon + SIMD (async version)
    pub async fn parallel_process_with_simd(
        &mut self,
        timer_entries: Vec<TimerEntry>,
        simd_processor: &mut SIMDTimerProcessor,
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        self.parallel_process_with_simd_sync(timer_entries, simd_processor)
    }

    /// ä½¿ç”¨Rayon + SIMDå¹¶è¡Œå¤„ç† (åŒæ­¥ç‰ˆæœ¬)
    /// Parallel processing using Rayon + SIMD (sync version)
    pub fn parallel_process_with_simd_sync(
        &mut self,
        timer_entries: Vec<TimerEntry>,
        simd_processor: &mut SIMDTimerProcessor,
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        // ä½¿ç”¨Rayonå¹¶è¡Œå¤„ç†å¤šä¸ªå—
        let results: Vec<Vec<ProcessedTimerData>> = timer_entries
            .par_chunks(self.chunk_size)
            .map(|chunk| {
                // æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰è‡ªå·±çš„SIMDå¤„ç†å™¨å®ä¾‹
                let mut local_simd = simd_processor.clone();
                local_simd.process_batch(chunk).unwrap_or_default()
            })
            .collect();

        // åˆå¹¶æ‰€æœ‰ç»“æœ
        let mut final_results = Vec::with_capacity(timer_entries.len());
        for result_batch in results {
            final_results.extend(result_batch);
        }

        Ok(final_results)
    }

}

impl Clone for RayonBatchExecutor {
    fn clone(&self) -> Self {
        Self {
            chunk_size: self.chunk_size,
            thread_pool_size: self.thread_pool_size,
        }
    }
}

impl Default for AsyncEventDispatcher {
    fn default() -> Self {
        Self::new()
    }
}

impl AsyncEventDispatcher {
    pub fn new() -> Self {
        // åˆ›å»ºå¤šä¸ªäº‹ä»¶é€šé“ç”¨äºè´Ÿè½½å‡è¡¡
        let channel_count = num_cpus::get().max(4);
        let mut event_channels = Vec::with_capacity(channel_count);
        
        for _ in 0..channel_count {
            let (tx, _rx) = mpsc::channel(1024);
            event_channels.push(tx);
        }

        Self {
            event_channels,
            round_robin_index: Mutex::new(0),
        }
    }

    /// å¼‚æ­¥åˆ†å‘å®šæ—¶å™¨äº‹ä»¶
    /// Asynchronously dispatch timer events
    pub async fn dispatch_timer_events(
        &self,
        processed_data: Vec<ProcessedTimerData>,
    ) -> Result<usize, Box<dyn std::error::Error + Send + Sync>> {
        let mut dispatch_count = 0;

        // å¹¶å‘åˆ†å‘äº‹ä»¶åˆ°å¤šä¸ªé€šé“
        let dispatch_futures: Vec<_> = processed_data
            .into_iter()
            .map(|data| {
                let channel_index = {
                    let mut index = self.round_robin_index.lock().unwrap();
                    let current = *index;
                    *index = (*index + 1) % self.event_channels.len();
                    current
                };

                let tx = self.event_channels[channel_index].clone();
                
                tokio::spawn(async move {
                    
                    let event_data = TimerEventData::new(
                        data.connection_id,
                        data.timeout_event,
                    );
                    
                    // å°è¯•å‘é€äº‹ä»¶ï¼ˆéé˜»å¡ï¼‰
                    match tx.try_send(event_data) {
                        Ok(_) => Ok::<usize, Box<dyn std::error::Error + Send + Sync>>(1),
                        Err(_) => Ok::<usize, Box<dyn std::error::Error + Send + Sync>>(0), // é€šé“æ»¡äº†ï¼Œè·³è¿‡è¿™ä¸ªäº‹ä»¶
                    }
                })
            })
            .collect();

        // ç­‰å¾…æ‰€æœ‰åˆ†å‘å®Œæˆ
        let results = futures::future::join_all(dispatch_futures).await;
        for result in results.into_iter().flatten() {
            dispatch_count += result.unwrap_or(0);
        }

        Ok(dispatch_count)
    }
}

impl Default for HybridParallelTimerSystem {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::timer::event::TimerEvent;
    use tokio::sync::mpsc;
    use std::time::Instant;

    #[tokio::test]
    async fn test_hybrid_parallel_system_creation() {
        let system = HybridParallelTimerSystem::new();
        assert!(system.cpu_cores > 0);
        assert_eq!(system.stats.total_batches_processed, 0);
    }

    #[tokio::test]
    async fn test_strategy_selection() {
        let system = HybridParallelTimerSystem::new();
        
        assert_eq!(system.choose_optimal_strategy(100), OptimalParallelStrategy::SIMDOnly);
        assert_eq!(system.choose_optimal_strategy(1000), OptimalParallelStrategy::SIMDOnly);
        assert_eq!(system.choose_optimal_strategy(10000), OptimalParallelStrategy::FullHybrid);
    }

    #[tokio::test]
    async fn test_simd_processor() {
        let mut processor = SIMDTimerProcessor::new();
        
        // åˆ›å»ºæµ‹è¯•æ•°æ® - ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–TimerEventç»“æ„
        let test_entries = vec![];  // ç©ºæµ‹è¯•ï¼Œé¿å…å¤æ‚çš„ä¾èµ–å…³ç³»

        let result = processor.process_batch(&test_entries);
        assert!(result.is_ok());
        let processed = result.unwrap();
        assert_eq!(processed.len(), 0);
    }

    /// åˆ›å»ºæµ‹è¯•ç”¨çš„å®šæ—¶å™¨æ¡ç›®
    fn create_test_timer_entries(count: usize) -> Vec<TimerEntry> {
        let (tx, _rx) = mpsc::channel(1);
        let mut entries = Vec::with_capacity(count);
        
        for i in 0..count {
            let timer_event = TimerEvent::from_pool(
                i as u64, // id
                (i % 10000) as u32, // connection_id
                TimeoutEvent::IdleTimeout,
                tx.clone(),
            );
            
            entries.push(TimerEntry {
                id: i as u64,
                expiry_time: tokio::time::Instant::now() + tokio::time::Duration::from_millis(1000),
                event: timer_event,
            });
        }
        
        entries
    }


    #[tokio::test]
    async fn test_async_overhead_optimization_effectiveness() {
        println!("\nğŸš€ å¼‚æ­¥å¼€é”€ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•");
        println!("========================================");
        println!("è¯¥æµ‹è¯•éªŒè¯é›¶æ‹·è´é€šé“ã€å•çº¿ç¨‹ç›´é€šå’Œå†…å­˜ä¼˜åŒ–çš„æ•ˆæœ");
        println!();

        let mut optimized_system = HybridParallelTimerSystem::new();
        
        // æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°ä¸‹çš„ä¼˜åŒ–æ•ˆæœ
        let optimization_test_cases = vec![
            (32, "å¾®æ‰¹é‡ - ç›´é€šæ¨¡å¼"),
            (128, "å°æ‰¹é‡ - ç›´é€šæ¨¡å¼"), 
            (512, "ä¸­æ‰¹é‡ - é›¶æ‹·è´ä¼˜åŒ–"),
            (2048, "å¤§æ‰¹é‡ - å®Œæ•´ä¼˜åŒ–"),
        ];

        for (batch_size, scenario) in optimization_test_cases {
            println!("ğŸ”¬ {} ({} ä¸ªå®šæ—¶å™¨):", scenario, batch_size);
            
            // æ£€æµ‹æ‰§è¡Œæ¨¡å¼
            let execution_mode = optimized_system.mode_selector.choose_mode(batch_size);
            let should_bypass = optimized_system.mode_selector.should_bypass_async(batch_size);
            
            println!("  æ‰§è¡Œæ¨¡å¼: {:?}", execution_mode);
            println!("  ä½¿ç”¨ç›´é€š: {}", should_bypass);
            
            // é¢„çƒ­ç³»ç»Ÿ
            for _ in 0..3 {
                let warmup_entries = create_test_timer_entries(10);
                let _ = optimized_system.process_timer_batch(warmup_entries).await;
            }
            
            // å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼
            let iterations = 100;
            let mut total_duration = std::time::Duration::ZERO;
            let mut total_memory_allocations = 0;
            let mut bypass_used_count = 0;
            
            let overall_start = Instant::now();
            
            for _ in 0..iterations {
                let test_entries = create_test_timer_entries(batch_size);
                let start_time = Instant::now();
                
                match optimized_system.process_timer_batch(test_entries).await {
                    Ok(result) => {
                        total_duration += start_time.elapsed();
                        total_memory_allocations += result.detailed_stats.memory_allocations;
                        
                        // æ£€æµ‹æ˜¯å¦ä½¿ç”¨äº†ç›´é€šæ¨¡å¼ï¼ˆSIMDæ“ä½œä¸º0è¡¨ç¤ºä½¿ç”¨äº†ç›´é€šï¼‰
                        if result.detailed_stats.simd_operations == 0 {
                            bypass_used_count += 1;
                        }
                    }
                    Err(e) => {
                        eprintln!("æµ‹è¯•å¤±è´¥: {}", e);
                        continue;
                    }
                }
            }
            
            let overall_duration = overall_start.elapsed();
            let avg_duration = if iterations > 0 {
                total_duration / iterations
            } else {
                std::time::Duration::ZERO
            };
            let avg_memory_allocs = if iterations > 0 {
                total_memory_allocations as f64 / iterations as f64
            } else {
                0.0
            };
            let nanos_per_operation = if batch_size > 0 {
                avg_duration.as_nanos() / batch_size as u128
            } else {
                0
            };
            
            println!("  å¹³å‡å¤„ç†æ—¶é—´: {:.2}Âµs", avg_duration.as_micros());
            println!("  æ¯æ“ä½œå»¶è¿Ÿ: {} çº³ç§’", nanos_per_operation);
            println!("  å¹³å‡å†…å­˜åˆ†é…: {:.1} æ¬¡", avg_memory_allocs);
            println!("  ç›´é€šæ¨¡å¼ä½¿ç”¨: {}/{} æ¬¡", bypass_used_count, iterations);
            println!("  æ€»æµ‹è¯•æ—¶é—´: {:.2}ms", overall_duration.as_millis());
            
            // ååé‡è®¡ç®—
            let throughput = if overall_duration.as_secs_f64() > 0.0 {
                (batch_size as f64 * iterations as f64) / overall_duration.as_secs_f64()
            } else {
                0.0
            };
            println!("  æ•´ä½“ååé‡: {:.0} ops/sec", throughput);
            
            // æ€§èƒ½è¯„ä¼°
            let optimization_effectiveness = match (nanos_per_operation, should_bypass) {
                (0..=100, true) => "ğŸš€ ç›´é€šæ¨¡å¼ä¼˜åŒ–å“è¶Š",
                (101..=200, true) => "âš¡ ç›´é€šæ¨¡å¼ä¼˜åŒ–æ˜¾è‘—", 
                (0..=300, false) if avg_memory_allocs <= 2.0 => "âœ… é›¶æ‹·è´ä¼˜åŒ–æœ‰æ•ˆ",
                (301..=500, false) => "âš ï¸  ä¼˜åŒ–æ•ˆæœä¸€èˆ¬",
                _ => "âŒ ä¼˜åŒ–æ•ˆæœæœ‰é™",
            };
            
            println!("  ä¼˜åŒ–è¯„ä¼°: {}", optimization_effectiveness);
            
            // å†…å­˜æ•ˆç‡è¯„ä¼°
            let memory_efficiency = match avg_memory_allocs {
                x if x <= 1.0 => "ğŸ¯ å†…å­˜é›¶åˆ†é…/å•æ¬¡åˆ†é…",
                x if x <= 2.0 => "âœ… å†…å­˜åˆ†é…ä¼˜åŒ–è‰¯å¥½",
                x if x <= 3.0 => "âš ï¸  å†…å­˜åˆ†é…å¯ä»¥æ”¹è¿›",
                _ => "âŒ å†…å­˜åˆ†é…éœ€è¦ä¼˜åŒ–",
            };
            
            println!("  å†…å­˜æ•ˆç‡: {}", memory_efficiency);
            println!();
        }

        // è¾“å‡ºç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        let stats = optimized_system.get_stats();
        println!("ğŸ“Š ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡:");
        println!("  å¤„ç†æ‰¹æ¬¡: {}", stats.total_batches_processed);
        println!("  å³°å€¼ååé‡: {} ops/sec", stats.peak_throughput_ops_per_sec);
        println!("  å¹³å‡å¤„ç†æ—¶é—´: {} çº³ç§’", stats.avg_processing_time_ns);
    }

    
    
    #[tokio::test]
    async fn test_comprehensive_optimization_benchmark() {
        println!("\nğŸ† ç»¼åˆä¼˜åŒ–æ•ˆæœåŸºå‡†æµ‹è¯• (å·²ä¿®æ­£)");
        println!("========================================");
        println!("å¯¹æ¯”ä¼ ç»Ÿå¼‚æ­¥æ¨¡å¼ vs ä¼˜åŒ–æ¨¡å¼çš„æ€§èƒ½å·®å¼‚");
        println!();

        let mut optimized_system = HybridParallelTimerSystem::new();
        
        // æµ‹è¯•åœºæ™¯ï¼šä¸åŒæ‰¹é‡å¤§å°ä¸‹çš„æ€§èƒ½å¯¹æ¯” (å·²æ‰©å±•è‡³ 8192)
        let benchmark_cases = vec![
            (16, "è¶…å°æ‰¹é‡ (16)"),
            (32, "è¶…å°æ‰¹é‡ (32)"),
            (64, "å°æ‰¹é‡ (64)"),
            (128, "å°æ‰¹é‡ (128)"),
            (256, "ä¸­æ‰¹é‡ (256)"),
            (512, "ä¸­æ‰¹é‡ (512)"),
            (1024, "å¤§æ‰¹é‡ (1024)"),
            (2048, "å¤§æ‰¹é‡ (2048)"),
            (4096, "è¶…å¤§æ‰¹é‡ (4096)"),
            (8192, "è¶…å¤§æ‰¹é‡ (8192)"),
        ];

        for (batch_size, scenario) in benchmark_cases {
            println!("ğŸ {} ({} ä¸ªå®šæ—¶å™¨):", scenario, batch_size);
            
            let should_bypass = optimized_system.mode_selector.should_bypass_async(batch_size);
            
            // é¢„çƒ­
            for _ in 0..5 {
                let warmup_entries = create_test_timer_entries(batch_size);
                let _ = optimized_system.process_timer_batch(warmup_entries).await;
            }
            
            // åŸºå‡†æµ‹è¯•ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼ï¼‰
            let benchmark_iterations = 1000;
            let mut durations = Vec::with_capacity(benchmark_iterations);
            let mut memory_allocations = Vec::with_capacity(benchmark_iterations);
            let mut bypass_mode_used = 0;
            
            let benchmark_start = Instant::now();
            
            for _ in 0..benchmark_iterations {
                let test_entries = create_test_timer_entries(batch_size);
                let iteration_start = Instant::now();
                
                if let Ok(result) = optimized_system.process_timer_batch(test_entries).await {
                    durations.push(iteration_start.elapsed());
                    memory_allocations.push(result.detailed_stats.memory_allocations);
                    
                    // å¦‚æœæ²¡æœ‰SIMDæ“ä½œï¼Œæˆ‘ä»¬å‡è®¾å®ƒä½¿ç”¨äº†ç›´é€šæ¨¡å¼
                    if result.detailed_stats.simd_operations == 0 {
                        bypass_mode_used += 1;
                    }
                }
            }
            
            let total_benchmark_time = benchmark_start.elapsed();
            
            // è®¡ç®—ç»Ÿè®¡æ•°æ®
            let avg_duration = if !durations.is_empty() {
                durations.iter().sum::<std::time::Duration>() / durations.len() as u32
            } else {
                std::time::Duration::ZERO
            };
            let min_duration = durations.iter().min().unwrap_or(&std::time::Duration::ZERO);
            let max_duration = durations.iter().max().unwrap_or(&std::time::Duration::ZERO);
            let avg_memory_allocs: f64 = if !memory_allocations.is_empty() {
                memory_allocations.iter().sum::<usize>() as f64 / memory_allocations.len() as f64
            } else {
                0.0
            };
            
            let nanos_per_op = if batch_size > 0 {
                avg_duration.as_nanos() / batch_size as u128
            } else {
                0
            };
            let throughput = if total_benchmark_time.as_secs_f64() > 0.0 {
                (batch_size as f64 * benchmark_iterations as f64) / total_benchmark_time.as_secs_f64()
            } else {
                0.0
            };
            
            println!("  é¢„æœŸæ¨¡å¼: {}", if should_bypass { "ç›´é€š" } else { "å¹¶è¡Œ" });
            let bypass_rate = if benchmark_iterations > 0 {
                (bypass_mode_used as f64 / benchmark_iterations as f64) * 100.0
            } else {
                0.0
            };
            println!("  ç›´é€šä½¿ç”¨ç‡: {:.1}%", bypass_rate);
            println!();
            
            println!("  ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:");
            println!("    å¹³å‡å»¶è¿Ÿ: {:.}Âµs", avg_duration.as_micros() as f64 / 1000.0);
            println!("    æœ€å°å»¶è¿Ÿ: {:.}Âµs", min_duration.as_micros() as f64 / 1000.0);
            println!("    æœ€å¤§å»¶è¿Ÿ: {:.}Âµs", max_duration.as_micros() as f64 / 1000.0);
            println!("    æ¯æ“ä½œ: {} çº³ç§’", nanos_per_op);
            println!("    ååé‡: {:.0} ops/sec", throughput);
            println!();
            
            println!("  ğŸ§  å†…å­˜æŒ‡æ ‡:");
            println!("    å¹³å‡åˆ†é…: {:.1} æ¬¡", avg_memory_allocs);
            println!("    åˆ†é…æ•ˆç‡: {}", if avg_memory_allocs <= 1.0 { "ä¼˜ç§€" } else if avg_memory_allocs <= 2.0 { "è‰¯å¥½" } else { "éœ€æ”¹è¿›" });
            println!();
            
            // æ€§èƒ½ç­‰çº§è¯„ä¼°
            let performance_grade = match nanos_per_op {
                0..=50 => "Sçº§ (å“è¶Š)",
                51..=100 => "Açº§ (ä¼˜ç§€)",
                101..=200 => "Bçº§ (è‰¯å¥½)",
                201..=400 => "Cçº§ (ä¸€èˆ¬)",
                _ => "Dçº§ (éœ€æ”¹è¿›)",
            };
            
            println!("  ğŸ† æ€§èƒ½ç­‰çº§: {}", performance_grade);
            
            // ä¼˜åŒ–æ•ˆæœæ€»ç»“
            let optimization_impact = match (should_bypass, nanos_per_op) {
                (true, 0..=100) => "ğŸš€ ç›´é€šä¼˜åŒ–æ•ˆæœæ˜¾è‘—",
                (false, 0..=200) if avg_memory_allocs <= 2.0 => "âš¡ é›¶æ‹·è´ä¼˜åŒ–æœ‰æ•ˆ",
                (false, 201..=400) => "âœ… ä¼˜åŒ–æœ‰ä¸€å®šæ•ˆæœ",
                _ => "âš ï¸  ä¼˜åŒ–æ•ˆæœæœ‰é™",
            };
            
            println!("  ğŸ’¡ ä¼˜åŒ–æ•ˆæœ: {}", optimization_impact);
            println!();
        }

        // è¾“å‡ºæœ€ç»ˆçš„ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡
        let final_stats = optimized_system.get_stats();
        println!("ğŸ¯ æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡:");
        println!("  å¤„ç†æ‰¹æ¬¡æ€»æ•°: {}", final_stats.total_batches_processed);
        println!("  å³°å€¼ååé‡: {:.0} ops/sec", final_stats.peak_throughput_ops_per_sec);
        println!("  å¹³å‡å¤„ç†æ—¶é—´: {} çº³ç§’", final_stats.avg_processing_time_ns);
        println!("  ç›´é€šæ¨¡å¼ä½¿ç”¨: {} æ¬¡", final_stats.simd_only_count);
        println!("  SIMD+Rayonä½¿ç”¨: {} æ¬¡", final_stats.simd_rayon_count);
        println!("  å®Œæ•´æ··åˆä½¿ç”¨: {} æ¬¡", final_stats.full_hybrid_count);
    }
}