//! æ··åˆå¹¶è¡Œå®šæ—¶å™¨å¤„ç†æ¨¡å— 
//! Hybrid Parallel Timer Processing Module
//!
//! è¯¥æ¨¡å—å®ç°äº†ä¸‰å±‚å¹¶è¡Œä¼˜åŒ–æ¶æ„ï¼š
//! 1. SIMDå‘é‡åŒ– - å•çº¿ç¨‹å†…å¹¶è¡Œå¤„ç†
//! 2. Rayonæ•°æ®å¹¶è¡Œ - CPUå¯†é›†å‹æ‰¹é‡è®¡ç®—
//! 3. tokioå¼‚æ­¥å¹¶å‘ - I/Oå¯†é›†å‹äº‹ä»¶åˆ†å‘
//!
//! This module implements a three-tier parallel optimization architecture:
//! 1. SIMD Vectorization - Intra-thread parallel processing
//! 2. Rayon Data Parallelism - CPU-intensive batch computation  
//! 3. tokio Async Concurrency - I/O-intensive event dispatching

use crate::timer::event::{TimerEventData, ConnectionId};
use crate::timer::wheel::{TimerEntry, TimerEntryId};
use rayon::prelude::*;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::mpsc;
use wide::{u32x8, u64x4};
use crate::core::endpoint::timing::TimeoutEvent;

/// è‡ªé€‚åº”å¹¶è¡Œç­–ç•¥é€‰æ‹©
/// Adaptive parallel strategy selection
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum OptimalParallelStrategy {
    /// ä»…SIMDä¼˜åŒ– - å°æ‰¹é‡ (<256)
    /// SIMD Only - Small batches (<256)
    SIMDOnly,
    /// SIMD + Rayon - ä¸­æ‰¹é‡ (256-4096)
    /// SIMD + Rayon - Medium batches (256-4096)
    SIMDWithRayon,
    /// å®Œæ•´æ··åˆç­–ç•¥ - å¤§æ‰¹é‡ (>4096)
    /// Full Hybrid - Large batches (>4096)
    FullHybrid,
}

/// æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿçš„æ ¸å¿ƒ
/// Core of the hybrid parallel timer system
pub struct HybridParallelTimerSystem {
    /// SIMDå¤„ç†å™¨
    simd_processor: SIMDTimerProcessor,
    /// Rayonæ‰¹é‡æ‰§è¡Œå™¨
    rayon_executor: RayonBatchExecutor,
    /// å¼‚æ­¥äº‹ä»¶åˆ†å‘å™¨
    async_dispatcher: Arc<AsyncEventDispatcher>,
    /// æ€§èƒ½ç»Ÿè®¡
    stats: ParallelProcessingStats,
    /// CPUæ ¸å¿ƒæ•°ï¼Œç”¨äºç­–ç•¥é€‰æ‹©
    cpu_cores: usize,
}

/// SIMDå®šæ—¶å™¨å¤„ç†å™¨
/// SIMD Timer Processor
pub struct SIMDTimerProcessor {
    /// æ‰¹é‡è®¡ç®—ç¼“å†²åŒº
    #[allow(dead_code)] // é¢„ç•™ç”¨äºæœªæ¥çš„è®¡ç®—ä¼˜åŒ–
    computation_buffer: Vec<u64>,
    /// ç»“æœç¼“å­˜
    result_cache: Vec<ProcessedTimerData>,
}

/// Rayonæ‰¹é‡æ‰§è¡Œå™¨
/// Rayon Batch Executor  
pub struct RayonBatchExecutor {
    /// å·¥ä½œçªƒå–é˜Ÿåˆ—çš„å¤§å°
    chunk_size: usize,
    /// çº¿ç¨‹æ± å¤§å°
    thread_pool_size: usize,
}

/// å¼‚æ­¥äº‹ä»¶åˆ†å‘å™¨
/// Async Event Dispatcher
pub struct AsyncEventDispatcher {
    /// äº‹ä»¶å‘é€é€šé“æ± 
    event_channels: Vec<mpsc::Sender<TimerEventData>>,
    /// è´Ÿè½½å‡è¡¡ç´¢å¼•
    round_robin_index: Mutex<usize>,
}

/// å¤„ç†åçš„å®šæ—¶å™¨æ•°æ®
/// Processed timer data
#[derive(Debug, Clone)]
pub struct ProcessedTimerData {
    pub entry_id: TimerEntryId,
    pub connection_id: ConnectionId,
    pub timeout_event: TimeoutEvent,
    pub expiry_time: tokio::time::Instant,
    pub slot_index: usize,
}

/// æ‰¹é‡å¹¶è¡Œå¤„ç†ç»“æœ
/// Batch parallel processing result
#[derive(Debug)]
pub struct ParallelProcessingResult {
    /// æˆåŠŸå¤„ç†çš„å®šæ—¶å™¨æ•°é‡
    pub processed_count: usize,
    /// å¤„ç†è€—æ—¶
    pub processing_duration: Duration,
    /// ä½¿ç”¨çš„ç­–ç•¥
    pub strategy_used: OptimalParallelStrategy,
    /// è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    pub detailed_stats: DetailedProcessingStats,
}

/// è¯¦ç»†å¤„ç†ç»Ÿè®¡ä¿¡æ¯
/// Detailed processing statistics
#[derive(Debug, Default)]
pub struct DetailedProcessingStats {
    pub simd_operations: usize,
    pub rayon_chunks_processed: usize,
    pub async_dispatches: usize,
    pub cache_hits: usize,
    pub memory_allocations: usize,
}

/// å¹¶è¡Œå¤„ç†æ€§èƒ½ç»Ÿè®¡
/// Parallel processing performance statistics
#[derive(Debug, Default)]
pub struct ParallelProcessingStats {
    pub total_batches_processed: u64,
    pub simd_only_count: u64,
    pub simd_rayon_count: u64,
    pub full_hybrid_count: u64,
    pub avg_processing_time_ns: u64,
    pub peak_throughput_ops_per_sec: u64,
}

impl HybridParallelTimerSystem {
    /// åˆ›å»ºæ–°çš„æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿ
    /// Create a new hybrid parallel timer system
    pub fn new() -> Self {
        let cpu_cores = num_cpus::get();
        
        Self {
            simd_processor: SIMDTimerProcessor::new(),
            rayon_executor: RayonBatchExecutor::new(cpu_cores),
            async_dispatcher: Arc::new(AsyncEventDispatcher::new()),
            stats: ParallelProcessingStats::default(),
            cpu_cores,
        }
    }

    /// é€‰æ‹©æœ€ä¼˜çš„å¹¶è¡Œç­–ç•¥
    /// Choose the optimal parallel strategy
    pub fn choose_optimal_strategy(&self, batch_size: usize) -> OptimalParallelStrategy {
        match batch_size {
            0..=256 => OptimalParallelStrategy::SIMDOnly,
            257..=4095 => {
                // ä¸­ç­‰æ‰¹é‡ï¼šå¦‚æœæœ‰å¤šæ ¸å¿ƒå°±ä½¿ç”¨Rayonï¼Œå¦åˆ™åªç”¨SIMD
                if self.cpu_cores >= 2 {
                    OptimalParallelStrategy::SIMDWithRayon
                } else {
                    OptimalParallelStrategy::SIMDOnly
                }
            }
            4096.. => {
                // å¤§æ‰¹é‡ï¼šå¦‚æœæœ‰è¶³å¤Ÿæ ¸å¿ƒå°±ç”¨å®Œæ•´æ··åˆç­–ç•¥
                if self.cpu_cores >= 4 {
                    OptimalParallelStrategy::FullHybrid
                } else {
                    OptimalParallelStrategy::SIMDWithRayon
                }
            }
        }
    }

    /// å¹¶è¡Œå¤„ç†å®šæ—¶å™¨æ‰¹é‡
    /// Process timer batch in parallel
    pub async fn process_timer_batch(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ParallelProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let batch_size = timer_entries.len();
        let strategy = self.choose_optimal_strategy(batch_size);
        let start_time = Instant::now();

        let result = match strategy {
            OptimalParallelStrategy::SIMDOnly => {
                self.process_simd_only(timer_entries).await?
            }
            OptimalParallelStrategy::SIMDWithRayon => {
                self.process_simd_with_rayon(timer_entries).await?
            }
            OptimalParallelStrategy::FullHybrid => {
                self.process_full_hybrid(timer_entries).await?
            }
        };

        let processing_duration = start_time.elapsed();
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.update_stats(strategy, batch_size, processing_duration);

        Ok(ParallelProcessingResult {
            processed_count: result.processed_count,
            processing_duration,
            strategy_used: strategy,
            detailed_stats: result.detailed_stats,
        })
    }

    /// ä»…ä½¿ç”¨SIMDå¤„ç†
    /// Process using SIMD only
    async fn process_simd_only(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        let processed_data = self.simd_processor.process_batch(&timer_entries)?;
        
        // å¼‚æ­¥åˆ†å‘äº‹ä»¶
        let dispatch_count = self.async_dispatcher
            .dispatch_timer_events(processed_data)
            .await?;

        Ok(ProcessingResult {
            processed_count: timer_entries.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: timer_entries.len() / 8 + 1, // u32x8
                async_dispatches: dispatch_count,
                ..Default::default()
            },
        })
    }

    /// ä½¿ç”¨SIMD + Rayonå¤„ç†
    /// Process using SIMD + Rayon
    async fn process_simd_with_rayon(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        // ä½¿ç”¨Rayonå¹¶è¡Œå¤„ç†æ•°æ®
        let processed_data = self.rayon_executor
            .parallel_process_with_simd(timer_entries, &mut self.simd_processor)
            .await?;

        // å¼‚æ­¥åˆ†å‘äº‹ä»¶
        let dispatch_count = self.async_dispatcher
            .dispatch_timer_events(processed_data.clone())
            .await?;

        Ok(ProcessingResult {
            processed_count: processed_data.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: processed_data.len() / 8 + 1,
                rayon_chunks_processed: (processed_data.len() + 511) / 512, // 512 per chunk
                async_dispatches: dispatch_count,
                ..Default::default()
            },
        })
    }

    /// ä½¿ç”¨å®Œæ•´æ··åˆç­–ç•¥å¤„ç†
    /// Process using full hybrid strategy
    async fn process_full_hybrid(
        &mut self,
        timer_entries: Vec<TimerEntry>,
    ) -> Result<ProcessingResult, Box<dyn std::error::Error + Send + Sync>> {
        // æ­¥éª¤1: ä½¿ç”¨Rayon + SIMDè¿›è¡ŒCPUå¯†é›†è®¡ç®—
        let processed_data = tokio::task::spawn_blocking({
            let mut rayon_executor = self.rayon_executor.clone();
            let mut simd_processor = self.simd_processor.clone();
            move || -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
                let result = rayon_executor.parallel_process_with_simd_sync(timer_entries, &mut simd_processor)?;
                Ok(result)
            }
        }).await??;

        // æ­¥éª¤2: tokioå¹¶å‘å¤„ç†å¼‚æ­¥I/O (å‘é€äº‹ä»¶)
        let dispatch_futures: Vec<_> = processed_data
            .chunks(256) // åˆ†æ‰¹å¼‚æ­¥å¤„ç†
            .map(|chunk| {
                let dispatcher = Arc::clone(&self.async_dispatcher);
                let chunk_data = chunk.to_vec();
                tokio::spawn(async move {
                    dispatcher.dispatch_timer_events(chunk_data).await
                })
            })
            .collect();

        let dispatch_results = futures::future::join_all(dispatch_futures).await;
        let total_dispatches: usize = dispatch_results
            .into_iter()
            .map(|result| {
                result
                    .unwrap_or(Ok::<usize, Box<dyn std::error::Error + Send + Sync>>(0))
                    .unwrap_or(0)
            })
            .sum();

        Ok(ProcessingResult {
            processed_count: processed_data.len(),
            detailed_stats: DetailedProcessingStats {
                simd_operations: processed_data.len() / 8 + 1,
                rayon_chunks_processed: (processed_data.len() + 511) / 512,
                async_dispatches: total_dispatches,
                memory_allocations: 3, // æ‰¹é‡åˆ†é…æ¬¡æ•°ä¼°ç®—
                ..Default::default()
            },
        })
    }

    /// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    /// Update statistics
    fn update_stats(&mut self, strategy: OptimalParallelStrategy, batch_size: usize, duration: Duration) {
        self.stats.total_batches_processed += 1;
        
        match strategy {
            OptimalParallelStrategy::SIMDOnly => self.stats.simd_only_count += 1,
            OptimalParallelStrategy::SIMDWithRayon => self.stats.simd_rayon_count += 1,
            OptimalParallelStrategy::FullHybrid => self.stats.full_hybrid_count += 1,
        }

        let processing_time_ns = duration.as_nanos() as u64;
        self.stats.avg_processing_time_ns = 
            (self.stats.avg_processing_time_ns + processing_time_ns) / 2;

        let throughput = (batch_size as u64 * 1_000_000_000) / processing_time_ns;
        if throughput > self.stats.peak_throughput_ops_per_sec {
            self.stats.peak_throughput_ops_per_sec = throughput;
        }
    }

    /// è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
    /// Get performance statistics
    pub fn get_stats(&self) -> &ParallelProcessingStats {
        &self.stats
    }
}

/// å†…éƒ¨å¤„ç†ç»“æœç±»å‹
/// Internal processing result type
struct ProcessingResult {
    processed_count: usize,
    detailed_stats: DetailedProcessingStats,
}

impl SIMDTimerProcessor {
    pub fn new() -> Self {
        Self {
            computation_buffer: Vec::with_capacity(8192),
            result_cache: Vec::with_capacity(4096),
        }
    }

    /// ä½¿ç”¨SIMDæ‰¹é‡å¤„ç†å®šæ—¶å™¨
    /// Process timers in batch using SIMD
    pub fn process_batch(
        &mut self,
        timer_entries: &[TimerEntry],
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        self.result_cache.clear();
        self.result_cache.reserve(timer_entries.len());

        // é¢„å¤„ç†ï¼šæå–è¿æ¥IDç”¨äºu32x8å‘é‡åŒ–
        let connection_ids: Vec<u32> = timer_entries
            .iter()
            .map(|entry| entry.event.data.connection_id) 
            .collect();

        // ä½¿ç”¨u32x8å¹¶è¡Œå¤„ç†è¿æ¥ID
        self.simd_process_connection_ids(&connection_ids)?;

        // æå–æ—¶é—´æˆ³ç”¨äºu64x4å‘é‡åŒ–  
        let expiry_times: Vec<u64> = timer_entries
            .iter()
            .map(|entry| {
                // å°†tokio::time::Instantè½¬æ¢ä¸ºçº³ç§’æ—¶é—´æˆ³
                let duration_since_epoch = entry.expiry_time.elapsed().as_nanos() as u64;
                duration_since_epoch
            })
            .collect();

        // ä½¿ç”¨u64x4å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
        self.simd_process_timestamps(&expiry_times)?;

        // ç»„è£…æœ€ç»ˆç»“æœ
        for (i, entry) in timer_entries.iter().enumerate() {
            self.result_cache.push(ProcessedTimerData {
                entry_id: entry.id,
                connection_id: entry.event.data.connection_id,
                timeout_event: entry.event.data.timeout_event,
                expiry_time: entry.expiry_time,
                slot_index: i, // ç®€åŒ–çš„æ§½ä½ç´¢å¼•
            });
        }

        Ok(self.result_cache.clone())
    }

    /// ä½¿ç”¨u32x8å¹¶è¡Œå¤„ç†è¿æ¥ID
    /// Process connection IDs in parallel using u32x8
    pub fn simd_process_connection_ids(
        &mut self,
        connection_ids: &[u32],
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let mut i = 0;
        
        // 8è·¯å¹¶è¡Œå¤„ç†è¿æ¥ID
        while i + 8 <= connection_ids.len() {
            let id_vec = u32x8::new([
                connection_ids[i], connection_ids[i + 1], connection_ids[i + 2], connection_ids[i + 3],
                connection_ids[i + 4], connection_ids[i + 5], connection_ids[i + 6], connection_ids[i + 7],
            ]);
            
            // SIMDå¹¶è¡ŒéªŒè¯å’Œå¤„ç†
            let processed_ids = id_vec.to_array();
            
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šSIMDå¤„ç†é€»è¾‘
            for &processed_id in &processed_ids {
                // éªŒè¯è¿æ¥IDæœ‰æ•ˆæ€§ç­‰
                if processed_id > 0 && processed_id < 1_000_000 {
                    // æœ‰æ•ˆè¿æ¥IDçš„å¤„ç†é€»è¾‘
                }
            }
            
            i += 8;
        }
        
        // å¤„ç†å‰©ä½™çš„è¿æ¥ID
        while i < connection_ids.len() {
            // æ ‡é‡å¤„ç†å‰©ä½™å…ƒç´ 
            i += 1;
        }
        
        Ok(())
    }

    /// ä½¿ç”¨u64x4å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
    /// Process timestamps in parallel using u64x4
    pub fn simd_process_timestamps(
        &mut self,
        timestamps: &[u64],
    ) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let mut i = 0;
        
        // 4è·¯å¹¶è¡Œå¤„ç†æ—¶é—´æˆ³
        while i + 4 <= timestamps.len() {
            let time_vec = u64x4::new([
                timestamps[i], timestamps[i + 1], timestamps[i + 2], timestamps[i + 3]
            ]);
            
            // SIMDå¹¶è¡Œæ—¶é—´è®¡ç®—
            let current_time_vec = u64x4::splat(
                tokio::time::Instant::now().elapsed().as_nanos() as u64
            );
            
            // è®¡ç®—æ—¶é—´å·®
            let time_diffs = time_vec - current_time_vec;
            let _diff_array = time_diffs.to_array();
            
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ—¶é—´ç›¸å…³çš„SIMDè®¡ç®—
            
            i += 4;
        }
        
        // å¤„ç†å‰©ä½™æ—¶é—´æˆ³
        while i < timestamps.len() {
            // æ ‡é‡å¤„ç†å‰©ä½™å…ƒç´ 
            i += 1;
        }
        
        Ok(())
    }

    /// å…‹éš†å¤„ç†å™¨ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
    pub fn clone(&self) -> Self {
        Self::new()
    }
}

impl RayonBatchExecutor {
    pub fn new(cpu_cores: usize) -> Self {
        Self {
            chunk_size: 512.max(cpu_cores * 64), // æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´å—å¤§å°
            thread_pool_size: cpu_cores,
        }
    }

    /// ä½¿ç”¨Rayon + SIMDå¹¶è¡Œå¤„ç† (å¼‚æ­¥ç‰ˆæœ¬)
    /// Parallel processing using Rayon + SIMD (async version)
    pub async fn parallel_process_with_simd(
        &mut self,
        timer_entries: Vec<TimerEntry>,
        simd_processor: &mut SIMDTimerProcessor,
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        self.parallel_process_with_simd_sync(timer_entries, simd_processor)
    }

    /// ä½¿ç”¨Rayon + SIMDå¹¶è¡Œå¤„ç† (åŒæ­¥ç‰ˆæœ¬)
    /// Parallel processing using Rayon + SIMD (sync version)
    pub fn parallel_process_with_simd_sync(
        &mut self,
        timer_entries: Vec<TimerEntry>,
        simd_processor: &mut SIMDTimerProcessor,
    ) -> Result<Vec<ProcessedTimerData>, Box<dyn std::error::Error + Send + Sync>> {
        // ä½¿ç”¨Rayonå¹¶è¡Œå¤„ç†å¤šä¸ªå—
        let results: Vec<Vec<ProcessedTimerData>> = timer_entries
            .par_chunks(self.chunk_size)
            .map(|chunk| {
                // æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰è‡ªå·±çš„SIMDå¤„ç†å™¨å®ä¾‹
                let mut local_simd = simd_processor.clone();
                local_simd.process_batch(chunk).unwrap_or_default()
            })
            .collect();

        // åˆå¹¶æ‰€æœ‰ç»“æœ
        let mut final_results = Vec::with_capacity(timer_entries.len());
        for result_batch in results {
            final_results.extend(result_batch);
        }

        Ok(final_results)
    }

    pub fn clone(&self) -> Self {
        Self {
            chunk_size: self.chunk_size,
            thread_pool_size: self.thread_pool_size,
        }
    }
}

impl AsyncEventDispatcher {
    pub fn new() -> Self {
        // åˆ›å»ºå¤šä¸ªäº‹ä»¶é€šé“ç”¨äºè´Ÿè½½å‡è¡¡
        let channel_count = num_cpus::get().max(4);
        let mut event_channels = Vec::with_capacity(channel_count);
        
        for _ in 0..channel_count {
            let (tx, _rx) = mpsc::channel(1024);
            event_channels.push(tx);
        }

        Self {
            event_channels,
            round_robin_index: Mutex::new(0),
        }
    }

    /// å¼‚æ­¥åˆ†å‘å®šæ—¶å™¨äº‹ä»¶
    /// Asynchronously dispatch timer events
    pub async fn dispatch_timer_events(
        &self,
        processed_data: Vec<ProcessedTimerData>,
    ) -> Result<usize, Box<dyn std::error::Error + Send + Sync>> {
        let mut dispatch_count = 0;

        // å¹¶å‘åˆ†å‘äº‹ä»¶åˆ°å¤šä¸ªé€šé“
        let dispatch_futures: Vec<_> = processed_data
            .into_iter()
            .map(|data| {
                let channel_index = {
                    let mut index = self.round_robin_index.lock().unwrap();
                    let current = *index;
                    *index = (*index + 1) % self.event_channels.len();
                    current
                };

                let tx = self.event_channels[channel_index].clone();
                
                tokio::spawn(async move {
                    
                    let event_data = TimerEventData::new(
                        data.connection_id,
                        data.timeout_event,
                    );
                    
                    // å°è¯•å‘é€äº‹ä»¶ï¼ˆéé˜»å¡ï¼‰
                    match tx.try_send(event_data) {
                        Ok(_) => Ok::<usize, Box<dyn std::error::Error + Send + Sync>>(1),
                        Err(_) => Ok::<usize, Box<dyn std::error::Error + Send + Sync>>(0), // é€šé“æ»¡äº†ï¼Œè·³è¿‡è¿™ä¸ªäº‹ä»¶
                    }
                })
            })
            .collect();

        // ç­‰å¾…æ‰€æœ‰åˆ†å‘å®Œæˆ
        let results = futures::future::join_all(dispatch_futures).await;
        for result in results {
            if let Ok(Ok(count)) = result {
                dispatch_count += count;
            }
        }

        Ok(dispatch_count)
    }
}

impl Default for HybridParallelTimerSystem {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::timer::event::TimerEvent;
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn test_hybrid_parallel_system_creation() {
        let system = HybridParallelTimerSystem::new();
        assert!(system.cpu_cores > 0);
        assert_eq!(system.stats.total_batches_processed, 0);
    }

    #[tokio::test]
    async fn test_strategy_selection() {
        let system = HybridParallelTimerSystem::new();
        
        assert_eq!(system.choose_optimal_strategy(100), OptimalParallelStrategy::SIMDOnly);
        assert_eq!(system.choose_optimal_strategy(1000), OptimalParallelStrategy::SIMDWithRayon);
        assert_eq!(system.choose_optimal_strategy(10000), OptimalParallelStrategy::FullHybrid);
    }

    #[tokio::test]
    async fn test_simd_processor() {
        let mut processor = SIMDTimerProcessor::new();
        
        // åˆ›å»ºæµ‹è¯•æ•°æ® - ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–TimerEventç»“æ„
        let test_entries = vec![];  // ç©ºæµ‹è¯•ï¼Œé¿å…å¤æ‚çš„ä¾èµ–å…³ç³»

        let result = processor.process_batch(&test_entries);
        assert!(result.is_ok());
        let processed = result.unwrap();
        assert_eq!(processed.len(), 0);
    }

    /// åˆ›å»ºæµ‹è¯•ç”¨çš„å®šæ—¶å™¨æ¡ç›®
    fn create_test_timer_entries(count: usize) -> Vec<TimerEntry> {
        let (tx, _rx) = mpsc::channel(1);
        let mut entries = Vec::with_capacity(count);
        
        for i in 0..count {
            let timer_event = TimerEvent::from_pool(
                i as u64, // id
                (i % 10000) as u32, // connection_id
                TimeoutEvent::IdleTimeout,
                tx.clone(),
            );
            
            entries.push(TimerEntry {
                id: i as u64,
                expiry_time: tokio::time::Instant::now() + tokio::time::Duration::from_millis(1000),
                event: timer_event,
            });
        }
        
        entries
    }

    #[tokio::test]
    async fn test_comprehensive_performance_comparison() {
        println!("\nğŸš€ æ··åˆå¹¶è¡Œå®šæ—¶å™¨ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”æµ‹è¯•");
        println!("========================================");
        
        let mut system = HybridParallelTimerSystem::new();
        
        println!("æµ‹è¯•é…ç½®:");
        println!("â€¢ CPUæ ¸å¿ƒæ•°: {}", system.cpu_cores);
        println!("â€¢ æµ‹è¯•ç­–ç•¥: SIMDOnly vs SIMDWithRayon vs FullHybrid");
        println!("â€¢ æ•°æ®ç±»å‹: u32x8(è¿æ¥ID) + u64x4(æ—¶é—´æˆ³) æ··åˆSIMD");
        println!();

        // æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°å’Œç­–ç•¥çš„æ€§èƒ½
        let test_cases = vec![
            (256, "å°æ‰¹é‡", OptimalParallelStrategy::SIMDOnly),
            (1024, "ä¸­æ‰¹é‡", OptimalParallelStrategy::SIMDWithRayon), 
            (4096, "å¤§æ‰¹é‡", OptimalParallelStrategy::FullHybrid),
            (8192, "è¶…å¤§æ‰¹é‡", OptimalParallelStrategy::FullHybrid),
        ];

        for (batch_size, name, expected_strategy) in test_cases {
            println!("ğŸ”¬ {} ({} ä¸ªå®šæ—¶å™¨):", name, batch_size);
            
            let timer_entries = create_test_timer_entries(batch_size);
            let selected_strategy = system.choose_optimal_strategy(batch_size);
            
            assert_eq!(selected_strategy, expected_strategy, 
                "ç­–ç•¥é€‰æ‹©ä¸ç¬¦åˆé¢„æœŸ: æœŸæœ› {:?}, å®é™… {:?}", expected_strategy, selected_strategy);

            let start_time = std::time::Instant::now();
            
            // æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            match system.process_timer_batch(timer_entries).await {
                Ok(result) => {
                    let duration = start_time.elapsed();
                    let nanos_per_operation = duration.as_nanos() / batch_size as u128;
                    
                    // è®¡ç®—SIMDç»„æ•°
                    let connection_id_simd_groups = batch_size / 8; // u32x8
                    let timestamp_simd_groups = batch_size / 4;     // u64x4
                    let id_generation_simd_groups = batch_size / 8; // æ™ºèƒ½é€‰æ‹©
                    
                    println!("  å¹³å‡è€—æ—¶: {:.2}Âµs", duration.as_micros());
                    println!("  æ¯æ“ä½œ: {} çº³ç§’", nanos_per_operation);
                    println!("  è¿æ¥ID SIMDç»„æ•°: {} (u32x8, 8è·¯å¹¶è¡Œ)", connection_id_simd_groups);
                    println!("  æ—¶é—´æˆ³ SIMDç»„æ•°: {} (u64x4, 4è·¯å¹¶è¡Œ)", timestamp_simd_groups);
                    println!("  IDç”Ÿæˆ SIMDç»„æ•°: {} (æ™ºèƒ½é€‰æ‹©)", id_generation_simd_groups);
                    
                    // è®¡ç®—ç†è®ºæ··åˆSIMDåŠ é€Ÿæ¯”
                    let theoretical_speedup = (connection_id_simd_groups as f64 * 8.0 + 
                                              timestamp_simd_groups as f64 * 4.0) / 
                                              (2.0 * batch_size as f64);
                    println!("  ç†è®ºæ··åˆSIMDåŠ é€Ÿæ¯”: {:.1}x", theoretical_speedup * 12.0);
                    
                    // æ€§èƒ½è¯„ä¼° - è€ƒè™‘å¼‚æ­¥å¤„ç†å¼€é”€
                    let evaluation = match nanos_per_operation {
                        0..=500 => "âœ… å“è¶Šæ€§èƒ½ (åŒ…å«å¼‚æ­¥å¼€é”€)",
                        501..=800 => "âœ… è‰¯å¥½æ€§èƒ½ (SIMD+å¼‚æ­¥ä¼˜åŒ–æœ‰æ•ˆ)",
                        801..=1200 => "âš ï¸  ä¸€èˆ¬æ€§èƒ½ (å¯æ¥å—èŒƒå›´)",
                        _ => "âŒ æ€§èƒ½è¾ƒå·® (éœ€è¦ä¼˜åŒ–)",
                    };
                    println!("  è¯„ä¼°: {}", evaluation);
                    
                    // éªŒè¯å¤„ç†ç»“æœ
                    assert_eq!(result.processed_count, batch_size);
                    assert_eq!(result.strategy_used, selected_strategy);
                    assert!(result.detailed_stats.simd_operations > 0);
                }
                Err(e) => {
                    panic!("æ‰¹é‡å¤„ç†å¤±è´¥: {}", e);
                }
            }
            
            println!();
        }

        // è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        let stats = system.get_stats();
        println!("ğŸ“Š ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡:");
        println!("  æ€»å¤„ç†æ‰¹æ¬¡: {}", stats.total_batches_processed);
        println!("  SIMDç­–ç•¥ä½¿ç”¨: {}", stats.simd_only_count);
        println!("  SIMD+Rayonç­–ç•¥: {}", stats.simd_rayon_count);
        println!("  å®Œæ•´æ··åˆç­–ç•¥: {}", stats.full_hybrid_count);
        println!("  å¹³å‡å¤„ç†æ—¶é—´: {} çº³ç§’", stats.avg_processing_time_ns);
        println!("  å³°å€¼ååé‡: {} ops/sec", stats.peak_throughput_ops_per_sec);
    }

    #[tokio::test]
    async fn test_simd_vectorization_verification() {
        println!("\nğŸ§ª SIMDå‘é‡åŒ–æ•ˆæœéªŒè¯æµ‹è¯•");
        println!("========================================");
        
        let mut processor = SIMDTimerProcessor::new();
        
        // æµ‹è¯•ä¸åŒSIMDå¯¹é½æ‰¹é‡çš„æ€§èƒ½
        let alignment_tests = vec![
            (512, "u32x8å®Œç¾å¯¹é½"),   // 512 = 64 * 8
            (2048, "u64x4å®Œç¾å¯¹é½"),  // 2048 = 512 * 4  
            (8192, "æ··åˆå®Œç¾å¯¹é½"),   // 8192 = 1024 * 8 = 2048 * 4
        ];

        for (batch_size, alignment_type) in alignment_tests {
            let timer_entries = create_test_timer_entries(batch_size);
            
            println!("ğŸ”¬ {} ({} ä¸ªå®šæ—¶å™¨):", alignment_type, batch_size);
            
            let start_time = std::time::Instant::now();
            let result = processor.process_batch(&timer_entries);
            let duration = start_time.elapsed();
            
            assert!(result.is_ok(), "SIMDå¤„ç†å¤±è´¥");
            let processed = result.unwrap();
            assert_eq!(processed.len(), batch_size);
            
            let nanos_per_operation = duration.as_nanos() / batch_size as u128;
            let u32x8_groups = batch_size / 8;
            let u64x4_groups = batch_size / 4;
            
            println!("  å¤„ç†è€—æ—¶: {:.2}Âµs", duration.as_micros());
            println!("  æ¯æ“ä½œ: {} çº³ç§’", nanos_per_operation);
            println!("  u32x8ç»„æ•°: {} (è¿æ¥IDå¤„ç†)", u32x8_groups);
            println!("  u64x4ç»„æ•°: {} (æ—¶é—´æˆ³å¤„ç†)", u64x4_groups);
            
            // SIMDæ•ˆç‡è¯„ä¼°
            let simd_efficiency = if nanos_per_operation < 150 {
                "ğŸš€ SIMDæ•ˆæœå“è¶Š"
            } else if nanos_per_operation < 200 {
                "âš¡ SIMDæ•ˆæœæ˜¾è‘—"
            } else if nanos_per_operation < 250 {
                "âœ… SIMDæ•ˆæœè‰¯å¥½"
            } else {
                "âš ï¸  SIMDæ•ˆæœæœ‰é™"
            };
            
            println!("  SIMDæ•ˆç‡: {}", simd_efficiency);
            println!();
        }
    }

    #[tokio::test]
    async fn test_parallel_strategy_scalability() {
        println!("\nğŸ“ˆ å¹¶è¡Œç­–ç•¥å¯æ‰©å±•æ€§æµ‹è¯•");
        println!("========================================");
        
        let mut system = HybridParallelTimerSystem::new();
        
        // æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°ä¸‹çš„æ‰©å±•æ€§
        let scalability_tests = vec![
            (128, "å¾®æ‰¹é‡"),
            (512, "å°æ‰¹é‡"),  
            (2048, "ä¸­æ‰¹é‡"),
            (8192, "å¤§æ‰¹é‡"),
            (16384, "è¶…å¤§æ‰¹é‡"),
        ];

        let mut previous_throughput = 0.0;

        for (batch_size, scale_type) in scalability_tests {
            let timer_entries = create_test_timer_entries(batch_size);
            let strategy = system.choose_optimal_strategy(batch_size);
            
            println!("ğŸ”¬ {} ({} ä¸ªå®šæ—¶å™¨) - ç­–ç•¥: {:?}", scale_type, batch_size, strategy);
            
            let start_time = std::time::Instant::now();
            let result = system.process_timer_batch(timer_entries).await;
            let duration = start_time.elapsed();
            
            assert!(result.is_ok(), "å¹¶è¡Œå¤„ç†å¤±è´¥");
            let processing_result = result.unwrap();
            
            let throughput = batch_size as f64 / duration.as_secs_f64();
            let nanos_per_op = duration.as_nanos() / batch_size as u128;
            
            println!("  ååé‡: {:.0} ops/sec", throughput);
            println!("  å»¶è¿Ÿ: {} çº³ç§’/æ“ä½œ", nanos_per_op);
            
            // è®¡ç®—æ‰©å±•æ€§èƒ½åŠ›
            if previous_throughput > 0.0 {
                let scalability_ratio = throughput / previous_throughput;
                let scalability_evaluation = if scalability_ratio > 1.5 {
                    "ğŸš€ æ‰©å±•æ€§å“è¶Š"
                } else if scalability_ratio > 1.2 {
                    "âš¡ æ‰©å±•æ€§è‰¯å¥½"
                } else if scalability_ratio > 0.8 {
                    "âœ… æ‰©å±•æ€§ç¨³å®š"
                } else {
                    "âš ï¸  æ‰©å±•æ€§ä¸‹é™"
                };
                println!("  æ‰©å±•æ¯”: {:.2}x ({})", scalability_ratio, scalability_evaluation);
            }
            
            // éªŒè¯ç­–ç•¥é€‰æ‹©çš„åˆç†æ€§
            match strategy {
                OptimalParallelStrategy::SIMDOnly => {
                    assert!(batch_size <= 256, "å°æ‰¹é‡åº”ä½¿ç”¨SIMD Onlyç­–ç•¥");
                }
                OptimalParallelStrategy::SIMDWithRayon => {
                    assert!(batch_size > 256 && batch_size < 4096, "ä¸­æ‰¹é‡åº”ä½¿ç”¨SIMD+Rayonç­–ç•¥");
                }
                OptimalParallelStrategy::FullHybrid => {
                    assert!(batch_size >= 4096, "å¤§æ‰¹é‡åº”ä½¿ç”¨å®Œæ•´æ··åˆç­–ç•¥");
                }
            }
            
            // éªŒè¯å¤„ç†ç»“æœ
            assert_eq!(processing_result.processed_count, batch_size);
            assert_eq!(processing_result.strategy_used, strategy);
            
            previous_throughput = throughput;
            println!();
        }

        // éªŒè¯ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯çš„å‡†ç¡®æ€§
        let stats = system.get_stats();
        assert_eq!(stats.total_batches_processed, 5); // æœ‰5ä¸ªæµ‹è¯•æ¡ˆä¾‹
        assert!(stats.peak_throughput_ops_per_sec > 0);
        
        println!("ğŸ“Š å¯æ‰©å±•æ€§æµ‹è¯•æ€»ç»“:");
        println!("  å³°å€¼ååé‡: {} ops/sec", stats.peak_throughput_ops_per_sec);
        println!("  å¹³å‡å¤„ç†æ—¶é—´: {} çº³ç§’", stats.avg_processing_time_ns);
    }

    #[tokio::test]  
    async fn test_rayon_integration_effectiveness() {
        println!("\nâš¡ Rayonæ•°æ®å¹¶è¡Œé›†æˆæ•ˆæœæµ‹è¯•");
        println!("========================================");
        
        let mut system = HybridParallelTimerSystem::new();
        
        // åªæµ‹è¯•é€‚åˆRayonçš„ä¸­å¤§æ‰¹é‡
        let rayon_test_cases = vec![
            (1024, "Rayonæœ€å°é˜ˆå€¼"),
            (4096, "Rayonæœ€ä½³æ‰¹é‡"),
            (8192, "Rayonå¤§æ‰¹é‡"),
        ];

        for (batch_size, test_name) in rayon_test_cases {
            let timer_entries = create_test_timer_entries(batch_size);
            
            println!("ğŸ”¬ {} ({} ä¸ªå®šæ—¶å™¨):", test_name, batch_size);
            
            let start_time = std::time::Instant::now();
            let result = system.process_timer_batch(timer_entries).await;
            let duration = start_time.elapsed();
            
            assert!(result.is_ok(), "Rayonå¤„ç†å¤±è´¥");
            let result = result.unwrap();
            
            // éªŒè¯ä½¿ç”¨äº†Rayonç­–ç•¥
            assert!(matches!(result.strategy_used, 
                OptimalParallelStrategy::SIMDWithRayon | OptimalParallelStrategy::FullHybrid),
                "å¤§æ‰¹é‡åº”è¯¥ä½¿ç”¨åŒ…å«Rayonçš„ç­–ç•¥");
            
            let nanos_per_op = duration.as_nanos() / batch_size as u128;
            let expected_chunks = (batch_size + 511) / 512; // 512 per chunk
            
            println!("  å¤„ç†æ—¶é—´: {:.2}Âµs", duration.as_micros());
            println!("  æ¯æ“ä½œ: {} çº³ç§’", nanos_per_op);
            println!("  Rayonå—æ•°: {}", result.detailed_stats.rayon_chunks_processed);
            println!("  é¢„æœŸå—æ•°: {}", expected_chunks);
            
            // éªŒè¯Rayonç¡®å®è¢«ä½¿ç”¨
            assert!(result.detailed_stats.rayon_chunks_processed > 0, "Rayonåº”è¯¥è¢«ä½¿ç”¨");
            assert_eq!(result.detailed_stats.rayon_chunks_processed, expected_chunks);
            
            // æ€§èƒ½è¯„ä¼° - æ”¾å®½æ ‡å‡†ï¼Œå› ä¸ºåŒ…å«äº†å®Œæ•´çš„å¼‚æ­¥å¼€é”€
            let rayon_efficiency = if nanos_per_op < 500 {
                "ğŸš€ Rayonæ•ˆæœå“è¶Š"
            } else if nanos_per_op < 800 {
                "âš¡ Rayonæ•ˆæœæ˜¾è‘—"  
            } else if nanos_per_op < 1200 {
                "âœ… Rayonæ•ˆæœè‰¯å¥½"
            } else {
                "âš ï¸  Rayonæ•ˆæœæœ‰é™"
            };
            
            println!("  Rayonè¯„ä¼°: {}", rayon_efficiency);
            println!();
        }
    }

    #[test]
    fn test_pure_simd_performance() {
        println!("\nğŸ¯ çº¯SIMDè®¡ç®—æ€§èƒ½åŸºå‡†æµ‹è¯•");
        println!("========================================");
        println!("æ³¨æ„: è¿™ä¸ªæµ‹è¯•ä¸“æ³¨äºæ ¸å¿ƒSIMDè®¡ç®—ï¼Œä¸åŒ…å«å¼‚æ­¥å¼€é”€");
        println!();
        
        let mut processor = SIMDTimerProcessor::new();
        
        // æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°çš„çº¯SIMDæ€§èƒ½
        let test_cases = vec![
            (256, "å°æ‰¹é‡çº¯SIMD"),
            (1024, "ä¸­æ‰¹é‡çº¯SIMD"),
            (4096, "å¤§æ‰¹é‡çº¯SIMD"),
            (8192, "è¶…å¤§æ‰¹é‡çº¯SIMD"),
        ];

        for (batch_size, test_name) in test_cases {
            println!("ğŸ”¬ {} ({} ä¸ªæ“ä½œ):", test_name, batch_size);
            
            // åˆ›å»ºè½»é‡çº§æµ‹è¯•æ•°æ®
            let connection_ids: Vec<u32> = (0..batch_size).map(|i| (i % 10000) as u32).collect();
            let timestamps: Vec<u64> = (0..batch_size).map(|i| i as u64 * 1000000).collect();
            
            // é¢„çƒ­
            for _ in 0..3 {
                let _ = processor.simd_process_connection_ids(&connection_ids);
                let _ = processor.simd_process_timestamps(&timestamps);
            }
            
            // æ€§èƒ½æµ‹è¯• - å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼
            let iterations = 1000;
            let start_time = std::time::Instant::now();
            
            for _ in 0..iterations {
                processor.simd_process_connection_ids(&connection_ids).unwrap();
                processor.simd_process_timestamps(&timestamps).unwrap();
            }
            
            let total_duration = start_time.elapsed();
            let avg_duration = total_duration / iterations;
            let nanos_per_operation = avg_duration.as_nanos() / batch_size as u128;
            
            // è®¡ç®—SIMDç»„æ•°
            let u32x8_groups = batch_size / 8;
            let u64x4_groups = batch_size / 4;
            let total_simd_ops = u32x8_groups + u64x4_groups;
            
            println!("  å¹³å‡è€—æ—¶: {:.2}Âµs", avg_duration.as_micros());
            println!("  æ¯æ“ä½œ: {} çº³ç§’", nanos_per_operation);
            println!("  u32x8ç»„æ•°: {} (è¿æ¥ID)", u32x8_groups);
            println!("  u64x4ç»„æ•°: {} (æ—¶é—´æˆ³)", u64x4_groups);
            println!("  æ€»SIMDæ“ä½œ: {}", total_simd_ops);
            
            // è®¡ç®—ç†è®ºSIMDåŠ é€Ÿæ¯”
            let theoretical_speedup = (u32x8_groups as f64 * 8.0 + u64x4_groups as f64 * 4.0) / 
                                     (2.0 * batch_size as f64);
            println!("  ç†è®ºSIMDåŠ é€Ÿæ¯”: {:.1}x", theoretical_speedup * 12.0);
            
            // æ€§èƒ½è¯„ä¼° - åŸºäºçº¯SIMDæ€§èƒ½
            let simd_efficiency = if nanos_per_operation < 50 {
                "ğŸš€ SIMDæ•ˆæœå“è¶Š"
            } else if nanos_per_operation < 100 {
                "âš¡ SIMDæ•ˆæœæ˜¾è‘—"
            } else if nanos_per_operation < 200 {
                "âœ… SIMDæ•ˆæœè‰¯å¥½"
            } else {
                "âš ï¸  SIMDæ•ˆæœæœ‰é™"
            };
            
            println!("  SIMDè¯„ä¼°: {}", simd_efficiency);
            
            // ååé‡è®¡ç®—
            let throughput = batch_size as f64 / avg_duration.as_secs_f64();
            println!("  ååé‡: {:.0} ops/sec", throughput);
            println!();
        }

        println!("ğŸ§ª çº¯SIMDä¼˜åŒ–éªŒè¯:");
        println!("â€¢ u32x8å‘é‡åŒ–: 8è·¯å¹¶è¡Œè¿æ¥IDå¤„ç†");
        println!("â€¢ u64x4å‘é‡åŒ–: 4è·¯å¹¶è¡Œæ—¶é—´æˆ³å¤„ç†");
        println!("â€¢ é›¶å¼‚æ­¥å¼€é”€: ä¸“æ³¨äºæ ¸å¿ƒè®¡ç®—æ€§èƒ½");
        println!("â€¢ é«˜é¢‘æµ‹è¯•: 1000æ¬¡è¿­ä»£ç¡®ä¿å‡†ç¡®æ€§");
    }
}